\input{packages.tex}  % <-- all the \usepackages
\input{pagestyle.tex} % <-- \pagestyle{body}
\input{acronyms.tex}  % <-- \pagestyle{body}
\input{macros.tex}    % <-- \x, \y, \D...
\author{Maël Valais}
\date{Updated on \today}
\title{Optimization of dictionaries structured in convolutional trees for sparse image representation - Master's Thesis}
\begin{document}
\input{titlepage} % Uncomment to show title page

\chapter*{Introduction \markboth{Introduction}{}}

\section{The need for sparse representations}

A sparse signal over some representation means that it can be expressed using only a few elements of the representation – in other words, sparse means with many zeros. Many applications ranging from machine learning to image denoising heavily rely on the property that we can summarize (or more exactly approximate) any signal using a proper sparse representation. The job of obtaining the raw\footnote{Any raw signal is actually represented by the canonical basis} signal from its sparse counterpart can be written in terms of an operator, often called \emph{dictionary} or \emph{transform}.

This chapter will review the main existing dictionaries that have been used for sparse representation, from non-adaptative transforms to learned dictionaries and through the intermediate over-complete dictionaries.

\section{Basis dictionaries}

The Discret Fourier Transform (DFT) is a classical example of operators used for sparsity. The DFT can be written as a matrix $\D$, where $\D$ denotes the dictionary. Finding the Fourier representation – which we will refer as the \emph{code} $\x$ – of an image $\y$ amounts to compute
$$\D^T\y = \x$$

\begin{figure}[!ht]
\subcaptionbox{Picture $\y$ with many discontinuities.}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_spacial.pdf}}
 \subcaptionbox{Result of applying the Fourier transform to $\y$}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_frequen.pdf}}
  \caption{Decomposition $\D^T\y=\x$ of a signal $\y$ using the dictionary $\D$ made of Fourier series. In $\x$, the middle coefficients are coding "large" features while the corner values are coding the details. The multiple white lines, ranging from the middle to the borders, outline the discontinuities caused by the blackboard edges; these coefficients are scattered (hence not sparse), which shows that the Fourier basis has representation problems in presence of discontinuities.} \label{fig_fourier}
\end{figure}

However, the Fourier transform is quite different from the representations we have in mind, namely the convolutional tree structure. First, it is a linear mapping and its corresponding matrix forms a basis. This representation is one-to-one in the sense that every code $\x$ is associated with a unique image $\y$. This basis is also orthogonal, meaning that the operator is stable (it does not amplify errors). % TODO Finally, the basis is normed, meaning that...

More importantly, Fourier has interesting properties that allow fast implementations. For example, applying the Matlab function \texttt{fft2} to the \cref{fig_fourier} takes less than a millisecond. If we assume $N$ to be the dimension of the concatenated\footnotemark[2] image, instead of computing a time-expensive $O(N^2)$ matrix-vector product, the fast Fourier transform only requires a $O(NlogN)$ computational cost.

\footnotetext[2]{Also referred as "vectorized"; we consider 2-D images as one-dimensional vectors. A $16 \times 16$ image would give a vector of dimension 256.}


The Fourier transform is widely used as an operator for sparse representations, mainly when dealing with smooth signals that can be approximated by a sum of sinusoids. However, this operator does not generally perform well on images since they contain many discontinuities that are far from being representable using sinusoids.

As an example, most of the coefficients of the code $\x$ in \cref{fig_fourier} are non-zero. The worst performance (in terms of sparsity) happens on the edges of the blackboard: the white vertical and horizontal lines that are full of high-value coefficients are trying\footnote{This effect is known as the Gibbs phenomenon.} to code the discontinuity, which is difficult to obtain with a Fourier basis.

Along with Fourier, many other non-adaptative dictionaries exist; we can mention the cosine transform which is at the core of JPEG compression. We can also cite the wavelet transform, used in JPEG2000 compression (for movies in theaters), and the curvelet transform, which is more tolerant to non-smooth images. Note that cosine, wavelet and curvelet transforms are stable and also have fast implementations.


\section{Frame-based dictionaries}

The basis-based dictionaries were generalized by using multiple bases to form an over-complete dictionary (called \emph{frame}) in \cite{chen_atomic_2001}. For example, combining a canonical basis with a cosine basis would allow to represent both peaks and sinusoids.

This method, although offering excellent performances for compressed sensing thanks to the fast implementations of the concatenated bases, still lacks adaptativity. Another approach coming from the data-science community adresses this problem: the dictionary learning.

\section{Learned dictionaries}

Like fixed-basis frames, learned dictionaries are over-complete. Instead of being fixed, atoms are based on a learning set of sample images
$$\Y = \begin{bmatrix} \y_1 & \dots & \y_S \end{bmatrix}$$
where $\Y$ is the concatenation of $S$ column vectors $\y_i$ (the images) of dimension $N$. The dictionary $\D$ is made of $K$ columns $\d_i$ (the atoms) such that the number of atoms is way larger than the dimension of a single image $\y_i$ ($ \gg K \gg N$):
$$\D = \begin{bmatrix} \d_1 & \dots & \d_K \end{bmatrix}$$

The codes $\X$ are defined by
$$\X = \begin{bmatrix} \x_1 & \dots & \x_S \end{bmatrix}$$
where $\X$ is the concatenation of $S$ column vectors $\x_i$ (the codes) of dimension $K$. The dictionary learning problem is defined as
\begin{align}
\underset{\D,\X}{\min}~ \lambda \lVert \X \rVert_1 + \lVert \D\X-\Y \rVert^2_F \tag{$DL$} \label{eq_dl}
\end{align}
where $\lVert . \rVert_F$ denotes the Frobenius norm and $\lVert . \rVert_1$ is the $l_1$ norm defined by $\lVert \X \rVert_1 = \sum_{i,j} |\x_i|$.

Many algorithms solving \eqref{eq_dl} alternatively optimize the dictionary $\D$ and the codes $\X$ so that atoms of $\D$ generate the sparsest representation $\x_i$ approximating the sample $\y_i$ for all $i = 1,\dots,S$:
$$\D\X \approx \Y$$

\Cref{fig_overcomplete_matrix} gives an idea of what it means to approximate an image $\y$ using the matrix-vector product of a sparse code $\x$ and a learned dictionary $\D$. For this example, $\y$ is represented by a linear combination of three atoms of $\D$: $\x$ is 3-sparse.

\begin{figure}[!ht] \centering
\includegraphics[width=0.80\textwidth]{figures/sparsity-matrix.pdf}
\caption{Matrix view of $\D\x$ when $\D$ is over-complete (much more columns than lines).}\label{fig_overcomplete_matrix}
\end{figure}



\section{The K-SVD algorithm}

Among the many existing alternating algorithms for dictionary learning like the Online dictionary learning algorithm or the K-means algorithm, the \ac{KSVD} algorithm is well known for its state-of-the-art performances in image denoising. Introduced in \cite{aharon_k-svd:_2006}, K-SVD is responsible for many concepts behind the PALMTREE algorithm developed in \cite{chabiron_optimization_2016}. This section summarizes the key elements of the K-SVD algorithm.

\ac{KSVD} does not actually solve \eqref{eq_dl}; instead, it gives a solution for a "stronger" problem in the sense that the level of sparsity in K-SVD is enforced by $\gamma$ in
\begin{align*}
\underset{\D,\X}{\min}~ & \lVert \D\X-\Y \rVert^2_F \tag{$DL_2$}\label{eq_dl_ksvd} \\
s.t.~& \lVert \x_i \rVert_0 \le \gamma \quad \forall i \text{ in } 1,\dots,S
\end{align*}
where $\lVert . \rVert_F$ denotes the Frobenius norm and $\lVert . \rVert_0$ is the pseudo-norm $l_0$ (also known as "counting function").

The \emph{Sparse Coding step} optimizes along $\x$; this step basically uses OMP (see \cref{alg_omp}) or any other pursuit algorithm. The \emph{Dictionary Update step} optimizes along $\D$. This is the step the authors of \cite{chabiron_optimization_2016} worked on. Note that the full K-SVD algorithm is in \cref{sec_ksvd_detail}, \cpageref{sec_ksvd_detail}.

\Cref{fig_ksvd} gives an example of what "learning $\D$" means with the K-SVD algorithm. As the image (a) is too big ($512 \times 512$) to be learned directly, we must chop it into many small patches of size $16 \times 16$. (b) shows some of these patches contained in $\Y$, which actually holds 10240 patches ($\Y$ has dimension $128 \times 10240$). The dictionary $\D$ of size $128 \times 128$ (it has 128 atoms) is shown in (c). $\D$ has been learned with a sparsity of 10 and the process took about five minutes (stopped after 20 iterations).

The dictionary in (c) can now be used for denoising the noisy image (a). If we wanted a dictionary for image recognition or compression, we would probably use many more images.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.40\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/ksvd/tableau_512x512.png}
	\caption{Image used for learning}
\end{subfigure}
\begin{subfigure}[b]{0.29\textwidth}\centering
	\includegraphics[width=0.7\textwidth]{figures/ksvd/patches.pdf}
	\caption{Image patches $\Y$}
\end{subfigure}
\begin{subfigure}[b]{0.29\textwidth}\centering
	\includegraphics[width=0.7\textwidth]{figures/ksvd/dictionary.pdf}
	\caption{Learned $\D$}\label{fig_ksvd_dict}
\end{subfigure}
\caption{Learning $\D$ on image patches using K-SVD.}\label{fig_ksvd_dict}
\end{figure}



% TODO add KSVD example of image

\subsection{Atoms scalability}
As the learned K-SVD dictionary relies on the $O(NK)$ matrix-vector product and that it must be over-complete ($K \gg N$) in order to achieve sparsity, we easily understand that $N$ cannot be big. When implemented, the K-SVD the sample images (also called patches) and atoms of the dictionary are generally at most $16 \times 16$. 

Dictionaries with bigger atoms are known as multi-resolution (also called multi-scale), meaning that the atoms can contain either details or large features in images.

[Talk about \cite{magoarou_learning_2015} (Elad team) and \cite{sulam_trainlets:_2016} (Gribonval team)]

\subsection{Atoms redundancy} \label{sec_atoms_redund}
The K-SVD algorithm is not translation-invariant in the sense that multiple atoms of the dictionary can have the same detail but slightly translated. The authors of \cite{mailhe_shift-invariant_2008} worked on making the update step invariant by translation, but it still needs a lot of tuning and finding the right parameters. 

Translation-invariant transforms are known as convolutional dictionaries. The convolution is basically the idea of trying to match the atom to every possible location of the image.

Many state-of-the-art machine learning techniques are based on trees of convolutions, like the Convolutional Neural Networks used in Deep learning \cite{lecun_deep_2015}.


\chapter{State of the art}

\section{Convolutional dictionaries}
[Didn't I say everything in \cref{sec_atoms_redund}???]

\section{The PALMTREE algorithm}

\subsection{Dictionary-as-a-tree model \label{sec_tree_model}}
The work studied in \cite{chabiron_toward_2015} and \cite{chabiron_optimization_2016} offers a different way of structuring the matrix $\D$. Instead of using "plain" atoms as columns and dealing with the matrix-vector product in $O(NK)$, $\D$ is defined by a convolutional tree structure $\T(\V,\E)$ where each edge $e$ of $\E$ is characterized by a kernel $\h^e$ and a support $\s^e$. The root $r$ and leaves $l$ of $\V$ allow to define a branch as the successive edges growing from the root to one of the leaves.

\begin{figure}[!ht]\centering
\includegraphics[width=\textwidth]{figures/tree.pdf}
\caption{View of a convolutional tree; the $\h^e$ and $\s^e$ are what define $\D$}\label{fig_tree}
\end{figure}

The target image $\y$ as well as the kernels $\h^e$ and supports $\s^e$ are vectors of dimension $N$. The supports $\s^e$ are only made of 0 (corresponding value in $\h^e$ must be 0) or 1 (corresponding value in $\h^e$ can be non-zero). The code $\x$ is a "big" vector of dimension $K$ with $K = N \times Leaves$; $\D$ is a matrix of dimensions $N \times K$ and $\y$ is a vector of dimension $N$.

In the following, we will use interchangeably the pixel set $1,\dots,N$ and image space definition $\P$.

The \cref{fig_example_kernel} displays examples of a standard kernel (left) and a kernel $\h^e$ (middle) associated with its support $\s^e$ (right). The gray squares represent the "ones" of the support while the kernel is represented by values.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.20\textwidth}\centering
\includegraphics[width=0.80\textwidth]{figures/kernel-exple.pdf}
\caption{Averaging kernel: fixed support and values}\label{fig_example_kernel}
\end{subfigure}
\begin{subfigure}[b]{0.79\textwidth}\centering
\includegraphics[width=0.80\textwidth]{figures/kernel-h_e.pdf}
\caption{Kernel $\h^e$ (left) and its associated support $\s^e$ (right).}\label{fig_example_kernel}
\end{subfigure}
\end{figure}

One atom of the dictionary $\D$ corresponds to the convolution of successive kernels on a given branch identified by the leaf $l$. We will denote that successive convolution by
$$\h^{*l} = \h^r * \dots * \h^l$$

Using the matrix notation\footnote{See \cref{sec_matrix_vs_tree}}, the dictionary-as-a-tree $\D$ applied to a code $\x$ approximates $\y$ as follows
$$\D\x \approx \y$$

The dictionary $\D$ can be written as a matrix (showed in \cref{sec_matrix_vs_tree}). This allows us to compare the performance of this dictionary against a simple matrix-vector product. Here is the dictionary-as-a-tree way of computing $\D\x$:

\begin{align}
	\D\x = \sum_{l \in \L} \x^l * \h^{*l}	 \label{eq_Dx_as_convolution}
\end{align}

The product $\D\x$ can be seen as a polynomial of degree $2 \times \text{depth}(\T)$. This implies that it is possibly extremely non-convex although being infinitely differentiable.

\subsection{The (FTL) problem}

The dictionary update step is defined as
\begin{align}
\underset{\substack{(\h^e)_{e}}}\min ~ & \lVert \D\x - \y \rVert_2^2 \tag{$FTL$} \label{eq_ftl}\\
\text{s.t.~} & \s^e_p=0 \Rightarrow \h^e_p = 0 \quad & \forall p=1,\dots,N ,~\forall e \in \E \label{eq_ftl_in_support} \\
 & \lVert \h^e \rVert \le \gamma & \forall e \in \E\label{eq_ftl_kernel_finite_nrj}
\end{align} The constraint in (\ref{eq_ftl_in_support}) guarantees that each kernel $\h^e$ has its non-zero values on the points of its support $\s^e$. Constraint (\ref{eq_ftl_kernel_finite_nrj}) ensures that the overall energy of every support is finite and prevents a specific kernel to "explode".

\eqref{eq_ftl} can be rewritten as an unconstrained problem by introducing a characteristic function $\chi_{\Dspace^e}$ defined for each edge $e$ as follows

\begin{align*}
	\chi_{\Dspace^e}(\h^e) = \begin{cases} 0 &\text{ if } \h^e \in \Dspace^e \\ +\infty & \ \text{otherwise}\end{cases} & \quad \text{with} \quad \Dspace^e = \begin{Bmatrix} \s^e_p=0 \Rightarrow \h^e_p = 0 \quad \forall p=1,\dots,N\\ \text{and }\lVert \h^e \rVert \le \gamma \end{Bmatrix}
\end{align*}

The unconstrained problem is
\begin{align}
\underset{\substack{(\h^e)_{e}}}\min ~ & \lVert \D\x - \y \rVert_2^2 + \sum_{e}\chi_{D_e} (\h^e) \tag{$FTL_2$} \label{eq_ftl2}
\end{align}

For later references, the objective function of \eqref{eq_ftl} will be denoted as
\begin{align}
\Phi[(\h^e)_{e \in \E}] = \lVert \D\x-\y \rVert^2_2
\end{align}

\subsubsection{Interesting properties of \eqref{eq_ftl}}

The convolution of sparse kernels in \eqref{eq_ftl} leads to theoretically extremely performant dictionaries. The computational cost of $\D\x$ is reduced\footnote{This is the simplified complexity. See \cref{sec_palmtree} for the actual complexity.} to $$O(|\E| N log N)$$ (with $|\E|$ the number of edges), which is very close to the performance of the fast Fourier transform.

However, $\D\x$ is possibly extremely non-convex (as mentioned in \cref{sec_tree_model}), suggesting that the optimization of \eqref{eq_ftl} is vain and that we would end up trapped by its many suboptimal critical points. Beyond proving that this problem is actually tractable, the authors of \cite{chabiron_optimization_2016} made the fascinating discovery that optimizing \eqref{eq_ftl} is possible and that the solutions given by PALMTREE were encouraging.

\subsection{The PALMTREE algorithm}\label{sec_palmtree}
[...todo...]

\subsection{Drawbacks}
As explained in \cite[p. 23]{chabiron_optimization_2016}, the main drawbacks to use this convolutional tree model for practical dictionary learning (as many other applications) are the hand-made parts of the model, leading to arbitrary and sub-optimal results. Among them are the design of the tree (number of children per node, depth) and the choice of the supports.
\subsubsection{Choice of the tree}
The paper chooses to use a fixed tree structure, meaning that the tree has been created \emph{ad-hoc} on a per-experiment basis, trying to mimic the frequency pyramid tiling of a curvelet decomposition. The number of leaves was also specifically chosen to match the number of atoms that was generated on the target image.

But getting an actual adaptative dictionary update step implies that designing the tree is made optimally and as part of the optimization process. This research axis has not been explored in this work and will be part of future studies.

\subsubsection{Choice of the supports}

As described in \cref{sec_tree_model}, the supports $\s^e$ are part of the dictionary-as-a-tree model. In \cite{chabiron_toward_2015}, the authors experimentally showed that using fixed supports for approximating is possible many kinds of atoms (curvelets, cosines). For the specific purpose of approximating existing atoms, the results turned out to be very promising. However, the proposed model was supposed to be able to approximate any kind of image.

So far, the supports have been handcrafted. \Cref{fig_example_kernel} shows an example of typical support layout chosen for the experiments in \cite{chabiron_optimization_2016}. We propose in this work to make the supports "adaptative", i.e. learning them during the optimization process.

\section{Learning the supports}
[todo: I explain what my contribution will be...]

\Cref{fig_fixed_vs_expected} shows an example of what we think would happen when allowing to add and remove elements: the support (b) would shape itself into (c) to better match the target atom.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/manual-better-support/target.pdf}
	\caption{}\label{fig_fixed_vs_expected_y}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.5\textwidth]{figures/manual-better-support/support.pdf}
	\caption{Generic kernel}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/manual-better-support/support-better.pdf}
	\caption{Expected kernel}
\end{subfigure}
\caption{Generic versus expected kernels. (a) is the target image made of one curvelet. (b) gives the locations of the support for one of the kernels of the tree. (c) shows a hand-made kernel that we expect to give better results as it follows the general direction of the curvelet}\label{fig_fixed_vs_expected}
\end{figure}

\Cref{fig_xp_fixed_vs_expected} experimentally confirms the assumption made in \Cref{fig_fixed_vs_expected}. We switched $\s^4$ from a generic support to a hand-made support that follows the general direction of $\y$ (\cref{fig_fixed_vs_expected_y}). The RMSE\footnotemark[1] is decreased by -16\% (lower is better).

\footnotetext[1]{Root mean square error. Let $\y^1$ and $\y^2$ two images of concatenated into a vector of dimension $N$, then $RMSE(\y^1,\y^2) = \frac{1}{N} \sqrt{\sum_{i=1}^N (y^1_i - y^2_i)^2}$}

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.09\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/tree_classic.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.39\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/xp_128x128_sc2_angl1_K3_S3_node4classic_approx.pdf}
	\caption{$\D\x$ with generic supports}
\end{subfigure}
\begin{subfigure}[b]{0.09\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/tree_expected.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.39\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/xp_128x128_sc2_angl1_K3_S3_node4expected_approx.pdf}
	\caption{$\D\x$ with hand-made $\s^4$}
\end{subfigure}
\caption{Visual enhancement when adapting a support.} \label{fig_xp_fixed_vs_expected}
\end{figure}




\chapter{Experiments}

This chapter covers the protocols and results of the various experiments aiming to learning the supports during the PALMTREE algorithm.

\section{Experiment protocol}
The following experiments are using a fixed single-branch tree of depth 4 associated with $\y$, $\h^e$, $\s^e$ and $\x^e$ of size $128 \times 128$. The figures only display the central $64 \times 64$ pixels as shown in \cref{fig_xp_explain}. The gain-per-added-point matrix (introduced in \cref{sec_gain_per_added_point}) is always computed with respect to one of the four supports $\s^e$.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.9\textwidth]{figures/xp_explain/target.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.9\textwidth]{figures/xp_explain/dictionary.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.9\textwidth]{figures/xp_explain/code.pdf}
	\caption{}
\end{subfigure}
\caption{Image $\y$, dictionary $\D$ and $\x$ used in experiments. $\y$ and $\x$ are given, $\D$ is learned.} \label{fig_xp_explain}
\end{figure}

\section{Using the gradient for choosing the elements to add}

The Orthogonal Matching Pursuit (OMP, detailed in \cref{alg_omp}) chooses which component $\x_i$ should be "turned on" at each iteration, finding the atom $d_i$ that has the greatest correlation with the residual $R$. This step (cf. \cref{alg_omp_pick_correlation}) uses the gradient $\nabla \Phi(\x)$, with $$\Phi(\x) = \lVert \D\x - \y \rVert^2$$


\begin{algorithm}[!ht]
    \caption{Orthogonal Matching Pursuit (OMP) algorithm for sparse approximation}\label{alg_omp}
  \begin{algorithmic}[1]
    \Input signal $\y$ of dimension $N$, dictionary of dim $N \times K$ with $K \gg N$
    \Output A $k$-sparse code $\x$ of dimension $K$
    \State \textbf{Initialization} $\bm{r}^{(0)} = \y$, $I=\{\}$
    \While{$i \leq k$}
      \State $l =  \underset{l = 1,\dots,l}{\arg\max} | \nabla \Phi(\x) |$ \label{alg_omp_pick_correlation}\Comment{find atom with max. correlation with $R$}
      \State $S = S \cup \{l\}$
      \State $\x = \underset{supp(\x) \subset S}{\arg\max} \Phi(\x)$
      \State $\bm{r}^{(i+1)} = \bm{r}^{(i)}-\x_l^T \d_l^{(i)}$
      \State $\hat{\y} = \hat{\y}+\langle \bm{r}^{(i)}, \d_{l}^{(i)} \rangle \d_{l}^{(i)}$
      \State $i = i + 1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

Our idea is to use the same principle when adding elements to supports $(\s^e)_{e \in \E}$: add a new element to one of the supports between two iterations of the PALMTREE algorithm, using the greatest gradient component information.

But is it actually correct to choose the element to be added using a strictly local information (the gradient)? Also, how to get the "full" gradient as the current gradient is only computed on the points of the support?

We will first review how the "full" gradient is computed and then describe a way to check that the gradient gives a good direction towards a critical point.


\subsection{Computing the "full" gradient}
Up to now, the gradient was computed using a convolution based on translations. It takes advantage of the sparsity of each kernel $\h^e$ as well as the fact that the proximal operator is projecting every kernel onto its support, meaning that only a few elements have to be computed.

We basically chose to use the fast Fourier transform ($\F$) for computing the convolution on the whole kernels:
\begin{align*}
	\nabla_{\h^f} \Phi((\h^e)_{e \in \E})&= \F^{-1}(\F(H^{e'})^* \text{.\^{}} \F(R))
\end{align*}
with ${}^*$ denoting the adjoint operator, $\text{.\^{}}$ the point-wise product, $\h^{f}$ the (todo) % TODO H^e?

\subsection{Validation of the "full" gradient}
We had some trouble when trying to compute the gradient on the full kernels $(\h^e)_{e \in \E}$ (not only on the supports element locations). To be sure that the computed "full" gradient was correct, we compared it to a finite-diffence gradient:

$$\lim_{\epsilon \rightarrow 0} \frac{\Phi((\h^e)_{e \in \E}+\epsilon e_i) - \Phi ((\h^e)_{e \in \E}) }{\epsilon} ~\overset{?}{=}~ \nabla_i \Phi ((\h^e)_{e \in \E})$$

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.30\textwidth}\centering
\includegraphics[width=\textwidth]{figures/verif_gradient/gradient.pdf}
\end{subfigure}
\begin{subfigure}[b]{0.30\textwidth}\centering
\includegraphics[width=\textwidth]{figures/verif_gradient/finite-diff.pdf}
\end{subfigure}
\begin{subfigure}[b]{0.35\textwidth}\centering
\includegraphics[width=\textwidth]{figures/verif_gradient/finite-diff-vs-grad.pdf}
\end{subfigure}
\caption{Checking that the "full" gradient is correct. We compare it to the finite-difference gradient and make the $\epsilon$ tend to 0.} \label{fig_verif_gradient}
\end{figure}

Figure \ref{fig_verif_gradient} compares the FFT-computed gradient to the finite-differences gradient (with $\epsilon=0.001$). The bottom left figure shows that the two gradients are very close in term of difference. The right figure shows that the finite-differences gradient converges to the "full" gradient, which confirms that our "full" gradient is correct.

\subsection{Gain-per-added-point to the support}\label{sec_gain_per_added_point}

One way to know if the "full" gradient leads to a good solution after many iterations, we designed the \cref{alg_gain_per_added_point} that tries every possible point added to one of the supports and see how much the objective function decreases. We named the resulting matrix \emph{gain-per-added-point} with the notation $\g^f$ ($f$ is the edge of the studied support).

$\g^f$ represents for each point $p \in \P$ the minimized objective function if an element were added on support kernel $\s^f$ at that specific location (cf. \cref{alg_gain_per_added_point}).

\begin{algorithm}[!ht]
    \caption{Gain-per-added-point for the support $\s^f$} \label{alg_gain_per_added_point}
  \begin{algorithmic}[1]
    \Input One chosen edge $f$ and a tree $\T$
    \Output $\g^f$, the gain per added point for support $\s^f$
    \State $(\k^e)_{e \in \E} = \underset{(\h^e)_{e \in \E}}{\arg\min}~ \Phi ((\h^e)_{e \in \E})$ \quad s.t.~$\h^e \in \Dspace^e$ \quad $\forall e \in \E$ \Comment{common starting point}
    \For{each point $p \in \P$}
    	\State $\s^f_p = 1$ \Comment{add element to support}
    	\State $\g^f_p = \underset{(\k^e)_{e \in \E}}{\min}~ \Phi ((\k^e)_{e \in \E})$ \quad s.t.~$\k^e \in \Dspace^e$ \quad $\forall e \in \E$
    	\State $\s^f_p = 0$ \Comment{remove element to support}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\Cref{fig_gain_n4} compares (c) the gain-per-added-point of $\s^4$ to (d) the absolute value of the partial gradient w.r.t $\h^4$ after a full minimization. It is interesting to see that they share the same direction and shape. After selecting 20 of their best values in (e) and (f), we notice that they almost match, which reinforces our hypothesis that the gradient could give the right information for widening supports.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.4\textwidth}\centering
\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_target.pdf}
\caption{Target $\y$}
\end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_approx.pdf}
\caption{Approximation $\D\x$} \label{fig_gain_n4_approx}
\end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix.pdf}
\caption{Gain-per-added-point for $\s^4$}
\end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4.pdf}
\caption{Partial gradient w.r.t $\h^4$}
\end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix_bestvalues.pdf}
\caption{Best values of above figure}
\end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4_bestvalues.pdf}
\caption{Best values of above figure}
\end{subfigure}
\caption{Minimization of a single branch tree with fixed supports. This experiment uses a single-branch tree with 4 kernels.}\label{fig_gain_n4}
\end{figure}


\begin{figure}[!ht]\centering
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n1/xp_128x128_sc2_angl1_K3_S3_node1_objmatrix_bestvalues.pdf}
	\caption{$\s^1$ gain}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n1/xp_128x128_sc2_angl1_K3_S3_node1_partgrad1_bestvalues.pdf}
	\caption{$\h^1$ gradient}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n2/xp_128x128_sc2_angl1_K3_S3_node2_objmatrix_bestvalues.pdf}
	\caption{$\s^2$ gain}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n2/xp_128x128_sc2_angl1_K3_S3_node2_partgrad2_bestvalues.pdf}
	\caption{$\h^2$ gradient}
	\end{subfigure}
\caption{Two other edges (1 and 2) for experiment in \cref{fig_gain_n4}. The image $\y$ and its approximation $\D\x$ are identical to \cref{fig_gain_n4}.}\label{fig_gain_n1_n2}
\end{figure}


\begin{figure}[!ht]\centering
	\begin{subfigure}[b]{0.22\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_target.pdf}
	\caption{Target $\y$}
	\end{subfigure}
	\begin{subfigure}[b]{0.22\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_approx.pdf}
	\caption{Approx. $\D\x$}
	\end{subfigure}
	\begin{subfigure}[b]{0.26\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_partgrad4_bestvalues.pdf}
	\caption{$\s^4$ gain}
	\end{subfigure}
	\begin{subfigure}[b]{0.26\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_objmatrix_bestvalues.pdf}
	\caption{$\h^4$ gradient}
	\end{subfigure}
\caption{Trying a different $\y$.}\label{fig_gain_tilted_n4}
\end{figure}


\subsubsection{Visual gain when adding an element}

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_target.pdf}
	\caption{Target image}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_approx.pdf}
\caption{Before adding}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_approx.pdf}
\caption{After adding}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_objmatrix.pdf}
\caption{Gain-per-added-point} \label{fig_gain_matrix}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_partgrad4.pdf}
\caption{Gradient before} \label{fig_grad_before}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_partgrad4.pdf}
\caption{Gradient after} \label{fig_grad_after}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_objmatrix_bestvalues.pdf}
\caption{Best values of \ref{fig_gain_matrix}}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_partgrad4_bestvalues.pdf}
\caption{Best values of \ref{fig_grad_before}}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_partgrad4_bestvalues.pdf}
\caption{Best values of \ref{fig_grad_after}}
\end{subfigure}
\caption{Effect of adding one point to the support $\s^4$. The "before" tree is already minimized. The "after" tree has been added a point where the gain-per-added-point matrix were minimal.} \label{fig_before_after_adding}
\end{figure}

The experiment shown in \cref{fig_before_after_adding} is based on the results of \cref{fig_gain_n4_approx}, where figures (a,b,d,e,g,h) are the same as the previous experiment. The figures (c,j,i) show what  happens if we add the minimum point of gain-per-added-point to $\s^4$ and continue the minimization.

In the "before" gradient (\cref{fig_grad_before}) every point of the support (the small squares) has a null gradient: in the direction of those nine points, the gradient is null, meaning that we are at an critical point.

We can observe that adding a point to the support (materialized by the \nth{10} small square), the highest gradient values "shift" to the south-west. Adding a point to the support has an effect on every surrounding points.

\subsection{Local versus far away information}

The experiment in \cref{fig_gain_n4} has only been done on a fully minimized tree, meaning that the gain-per-added-point would be "close" to what the gradient would indicate. But we could wonder if the gradient stays the same throughout all iterations; compared to the converged gain-per-added-point which is a "far away" information, the gradient might not lead to that gain.

The experiment shown in \cref{fig_iter_gain_vs_grad} compares the best values of gain-per-added-point to gradients snapshots taken during the minimization. This is reassuring: whatever the iteration, the gradient best values keep close to the gain-per-added-point best values.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.49\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix_bestvalues.pdf}
\caption{Gain-per-added-point matrix}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}\centering
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter1_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 1}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter8_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 8}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter20_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 20}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter200_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 200}
	\end{subfigure}
\end{subfigure}
\caption{Is the gradient giving the same information depending on the iteration? The conditions are the same as \cref{fig_simple_approx}. Only the best values are displayed.}\label{fig_iter_gain_vs_grad}
\end{figure}




\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.4\textwidth}\centering
	\begin{subfigure}[b]{1\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/variable_support/xp_128x128_sc2_angl1_K3_S3_node4_variable_target.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{1\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/variable_support/xp_128x128_sc2_angl1_K3_S3_node4_variable_approx.pdf}
	\end{subfigure}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}\centering
\includegraphics[width=0.37\textwidth]{figures/variable_support/support.pdf}
\end{subfigure}
\caption{Minimization using a variable support. Starting from simple diracs, the best element (using greatest gradient) is added to one of the supports every 5 iterations, up to 36 elements (same number as the fixed support in \cref{fig_gain_n4}).} \label{fig_variable_support}
\end{figure}







\clearpage
\addcontentsline{toc}{chapter}{Appendix}
\appendix

\chapter{Miscellaneous}


\section{Link between convolutional tree and dictionary matrix}\label{sec_matrix_vs_tree} % $\bm{D} in \section{} bugs

Figure \ref{fig_block_circular} outlines the relation between the tree and the matrix form. The dictionary can be written using the notation
$$\D = \begin{bmatrix}C^5 & C^6\end{bmatrix} \begin{bmatrix}C^1 & C^2 & 0 & 0 \\0 & 0 & C^3 & C^4\end{bmatrix}$$
with $C^i$ the block-circular matrix for $\h^i$; it represents the final convolution $\h^{*l}*\x^l$ when using matrices.

\begin{figure}[!ht] \centering
\begin{subfigure}[b]{0.30\textwidth}\centering
\includegraphics[width=\textwidth]{figures/pov-tree.pdf}
\caption{Convolutional tree}
\end{subfigure}
\begin{subfigure}[b]{0.69\textwidth}\centering
\includegraphics[width=\textwidth]{figures/pov-matrix.pdf}
\caption{Best values of above figure}
\end{subfigure}
\caption{This figure shows how to go from the tree defined by its kernels $(\h^e)_{e \in \E}$ to a matrix $\D$} \label{fig_block_circular}
\end{figure}

\section{Detail of the K-SVD algorithm} \label{sec_ksvd_detail}
\begin{algorithm}
    \caption{K-SVD (K-Singular Value Decomposition) algorithm for \eqref{eq_dl_ksvd}} \label{alg_ksvd}
  \begin{algorithmic}[1]
    \Input signal samples $\y_i$ of dimension $N$ forming the $S$ columns of $\Y$
    \Output dictionary $\D$ of dim. $K \times N$ with $K \gg N$
    \State \textbf{Initialization} Initialize $\D$ s.t. every column is $l^2$ normalized
    \While{convergence}
	\State For each image $\y_i$, solve \Comment{\textbf{Sparse Coding step}} \label{alg_ksvd_sparse_coding}
		\begin{align*}
			\quad \min_{\x_i}~ & \lVert \y_i-\D\x_i\rVert^2 \quad \text{s.t.}~ \lVert \x_i \rVert_0 \le \gamma
		\end{align*}
	\State $\D^{old} = \D$
	\State For each atom $\d_k^{old}$ of $\D^{old}$, \Comment{\textbf{Dictionary Update step}} \label{alg_ksvd_dict_update}
	\begin{enumerate}[leftmargin=15mm,label=(\alph*)]
		\item find the images $\y_i$ that use the atom $\d_k^{old}$, meaning $\x_i^k \ne 0$ \label{item_atom_used}
		\item compute the error matrix associated to this atom,
		\begin{align*}
			\bm{E}_k = \Y-\sum_{i\ne k} \x_i^T\d_i^{old}
		\end{align*}
		\item keep only columns of $\bm{E}_k$ found in \ref{item_atom_used}
		\item Apply SVD decomposition $\bm{E}_k - \bm{U} \Delta V^T$. The first column of $\bm{U}$ becomes the new $\d_k$. $\x_k$ is updated using the first column of $\bm{V}$ multiplied by $\Delta(1,1)$ (highest Eigen value)
	\end{enumerate}
    \EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Why is sparse coding used for denoising?}

The figure \ref{sparse_reduce_noise} shows a noisy signal $\y$ living in a high-dimensional $N$ and defined by
$$\y=\hat{\y} + \bm{b}$$
with $b$ following a centered Gaussian distribution $\mathcal{N}(0,\sigma^2I)$ and $\hat{\y}$ the noise-less signal. We see that the distance $||b||$ is always lower than the projected distance.

This is because the deviation in the $N$ dimensional space can be written as
\begin{align*}
\sigma^2(b) =& \mathbb{E}\left[\lVert b-\mathbb{E}(b) \rVert^2 \right]\\
=& \mathbb{E}\left(\lVert b \rVert^2 \right)\\
=& ... \\ % TODO finish useless proof
=& \frac{1}{N}\lVert b \rVert^2
\end{align*}
which gives 
$$ \lVert b \rVert = \sigma\sqrt{N} $$
When projected to the $K$ dimensional space, the noise deviation becomes
$$\lVert b \rVert = \sigma\sqrt{K} $$
which is much better than the previous distance, provided that $K \gg N$. 

\begin{figure}[!ht]\centering
\includegraphics[width=0.4\textwidth]{figures/sparse-reduce-noise.pdf}
\caption{When projected onto a lower dimensional space, the standard derivation of the additive Gaussian noise $b \sim \mathcal{N}(0,\sigma^2)$ will be greatly reduced if $K \gg N$. The sparser the representation the better the denoising. \label{sparse_reduce_noise}}
\end{figure}

\section{Why $FT(\widetilde{A}) = FT(A)^*$}
\begin{align*}
(\widehat{\widetilde{A}})_{m,n} =& \sum_{k=1}^M \sum_{l=1}^N \widehat{A}_{k,l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
=& \sum_{k=1}^M \sum_{l=1}^N A_{-k,-l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
\shortintertext{By changing variables $k'=-k$ and $l'=-l$, we get:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\shortintertext{And thanks to the $(M,N)$ periodicity of $A$, which means that $A_{i,j}=A_{i+kM,j+lN}$, $\forall (k,l) \in \mathbb{N}^2$, letting us with:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k'+M,l'+N} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
\shortintertext{With a second change of variables $k''=k'+M$ and $l''=l'+N$:}
=& \sum_{k''=-M+M}^{-1+M} \sum_{l''=-N+N}^{-1+N} A_{k'',l''} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
=& \sum_{k'=1}^{M} \sum_{l'=1}^{N} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\end{align*}
% TODO Proof not finished; this proof might be useless/overkill in my thesis...


\printbibliography

\end{document}



