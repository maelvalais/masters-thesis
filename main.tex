\input{packages_config.tex} % <-- all the \usepackages
\input{mystyle.tex}
\input{acronyms.tex}
\input{macros.tex}
\author{Maël Valais}
\date{Updated on \today}
\title{Optimization of dictionaries structured in convolutional trees for sparse image representation - Master's Thesis}
\begin{document}
\input{titlepage} % Uncomment to show title page
\pagestyle{body}
\tableofcontents

\chapter{Introduction}

\section{The need for sparse representations}

A sparse signal over some representation means that it can be written with as few information as possible – in other words, sparse means with many zeros. Many applications spanning from machine learning to image denoising heavily rely on the property that we can summarize – or more precisely approximate – any signal using a proper representation. The job of obtaining the raw\footnote{Any raw signal is actually represented by the canonical basis} signal from its sparse counterpart takes the form of an operator, often called dictionary or transform.

This chapter will review the main existing dictionaries in use for sparse representation, spanning from non-adaptative transforms to learned dictionaries and going through the intermediate over-complete dictionaries.

\section{Non-adaptative dictionaries}

The Fourier transform is a classical example of operators used for sparsity. In figure \ref{fig_fourier}, the left picture $y$ has been represented using a Fourier basis, which can be written as a matrix $D$. The right picture shows the coefficients $x$ such that
$$D^Ty = x$$


\begin{figure}[!ht]
\subcaptionbox{Picture $y$ with many discontinuities.}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_spacial.pdf}}
 \subcaptionbox{Result of applying the Fourier transform to $y$}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_frequen.pdf}}
  \caption{Decomposition $D^Ty=x$ of a signal $y$ using the dictionary $D$ made of Fourier series. In $x$, the middle coefficients are coding for "large" features while the corner values are coding for the details. The multiple white lines, spanning from the middle to the borders, outline the discontinuities caused by the edges of the blackboard; the coefficients are scattered (not sparse), which shows that Fourier has trouble when dealing with discontinuities.} \label{fig_fourier}
\end{figure}

But the Fourier transform is quite different from the representations we have in mind, namely the convolutional tree structure. First, it's a linear mapping and its associated matrix forms a basis. This implies bijectivity: every code $x$ has a unique image $y$. Also, Fourier's basis is orthogonal, meaning that the operator is stable (it does not amplify errors). Finally, the basis is normed, meaning that ... % TODO (HELP) explain normed basis

More importantly, Fourier has interesting properties that allow fast implementations. For example, applying the matlab function \texttt{fft2} on the \cref{fig_fourier} is extremely fast. Instead of computing an time-expensive $O(N^2)$\footnote{$N$ is the dimension of the "flattened" image, if we consider the image as a vector} matrix-vector product, the fast Fourier transform only require a $O(NlogN)$ time.

The Fourier transform is widely used as an operator for sparse representations, mainly when dealing with smooth signals that can be approximated by a sum of sines. But this operator does not do well on images: they contain many discontinuities and are far from being representable using sines. Most of the coefficients of code $x$ in \cref{fig_fourier} are non-zero. But something worse happens on the edges of the blackboard: the white vertical and horizontal lines that are full of high-value coefficients are trying to code for the discontinuity, which is quite far from possible in Fourier basis.

Along with Fourier, many other non-adaptative dictionaries exist; we can mention the cosine transform which is at the core of JPEG compression. We can also cite the wavelet transform, used in JPEG2000 compression (for movies in theaters), and the curvelet transform, which a bit more tolerant to non-smooth images. And as for Fourier, wavelet and curvelet transforms both have fast implementations.


\section{Adaptative dictionaries}
Right before the first learned dictionaries, \cite{chen_atomic_2001} extended the basis-based dictionaries using multiple basis to form an over-complete dictionary. Doing so would increase the expressivity of such representations, although making the inverse transform less obvious. % TODO overcomplete implies what?

Learned dictionaries take the same bet of over-completeness. But instead of choosing multiple fixed basis, we optimize the dictionary matrix so that the atoms (other name for the columns of the dictionary matrix) generate the sparsest representations for a given set of images.

The \cref{fig_overcomplete_matrix} shows the matrix-vector product of a sparse code $x$ and a learned $D$, giving an approximation of $y$. For this example, $y$ will be represented by a linear combination of three atoms of $D$.


\begin{figure}[!h] \centering
\includegraphics[width=0.5\textwidth]{figures/sparsity-matrix.pdf}
\caption{Matrix view of $Dx$ when $D$ is over-complete (much more columns than lines). $y$ can be represented using a few atoms of $D$.} \label{fig_overcomplete_matrix}
\end{figure}


Given a set of sample images $Y = [ y_1 y_2 \dots y_S ]$ (images are "flattened" into vectors $y_i$ of size $N$), the dictionary learning optimization problem is defined as

\begin{equation*}  \begin{aligned}
(DL) && \underset{D,X}{\min} \lambda \lVert x \rVert_1 + \lVert DX-Y \rVert^2_2 
\end{aligned} \label{eq_dict_learn} \end{equation*}

% TODO WARNING! ||X||_1 doesn't exist, ||Y||_2 either
with $\lVert . \rVert_2$ denoting the standard euclidian $l_2$ norm, $\lVert . \rVert_1$ the $l_1$ norm defined by $\lVert x \rVert_1 = \sum_{i,j} |x_i|$, $X = [ x_1 \dots x_s ]$ the matrix containing the sparse representations $x_i$ of size $K$ for all images in $Y$ and $D \in \mathcal{M}^{K \times N}$ the dictionary.

\section{K-SVD algorithm}

\ac{KSVD}, first introduced in \cite{aharon_k-svd:_2006}, is a well known algorithm for dictionary learning (except that it does not actually solve the above problem \cref{eq_dict_learn}). As many of the concepts behind the PALMTREE algorithm developed in \cite{chabiron_optimization_2016} come from the K-SVD algorithm, this section will introduce it. % TODO Why KSVD != dict. learning problem 

Here is the problem that K-SVD is solving, which is a "stronger" version of (DL) in the sense that the level of sparsity in K-SVD is enforced:
\begin{equation*}  \begin{aligned}
(DL_{\text{strong}}) && \underset{D,X}{\min} & \lVert DX-Y \rVert^2_F \\
&& s.t. & \lVert x_i \rVert_0 \le \gamma & \quad \forall i \text{ in } 1..S
\end{aligned} \label{eq_dict_learn} \end{equation*}

with $F$ the Frobenius norm and $x_i$ the i-th column of $X$.

\ac{KSVD} learns the dictionary by alternatively optimizing $x$ and $D$. Optimizing $x$ is the sparse coding phase, and optimizing $D$ is the dictionary update phase.

\begin{algorithm}
    \caption{K-SVD (K-Singular Value Decomposition) algorithm for (DL)}
  \begin{algorithmic}[0]
    \Input signal samples $(y_i)_{i=1..N}$ forming columns of $Y \in \mathbb{R}^{S \times N}$
    \Output dictionary $D \in \mathbb{R}^{K \times N}$ with $K>>N$
    \State \textbf{Initialization} Initialize $D$ s.t. every column is $l^2$ normalized
    \While{convergence}
	\State For each image $i$, solve \Comment{Sparse coding step}
		\begin{align}
			\quad \min_{x_i} & \lVert y_i-Dx_i\rVert^2\\
			s.t. & \lVert x_i \rVert_0 \le \gamma \notag
		\end{align}
	\State Set $D^{old} \leftarrow D$
	\State For each atom $k$ of $D^{old}$, \Comment{Dictionary update step}
	\begin{enumerate}[leftmargin=15mm,label=(\alph*)]
		\item find the images $y_i$ that use the atom $d_k$, meaning $x_i^k \ne 0$ \label{item_atom_used}
		\item compute the error matrix associated to this atom,
		\begin{align}
			E_k =& Y-\sum_{i\ne k} {d_i}^Tx_i
		\end{align}
		\item keep only columns of $E_k$ found in \ref{item_atom_used}
		\item Apply SVD decomposition $E_k - U \Delta V^T$. The first column of $U$ becomes the new $d_k$. $x_k$ is updated using the first column of $V$ multiplied by $\Delta(1,1)$ (highest Eigen value)
	\end{enumerate}
    \EndWhile
  \end{algorithmic}
\end{algorithm}
\section{The seek for fast transforms}

... talk about the problem of need for small patches and resolution limits in KSVD % TODO KSVD problems

As K-SVD relies on the $O(NK)$ matrix-vector product and that a dictionary achieves sparsity as long as $K >> N$, we easily understand that $N$ cannot be big. When implemented, K-SVD atoms (called patches because of their size) are at most $16 \times 16$.

Also, the K-SVD is translation-sensitive in the sense that multiple atoms of the dictionary are only different by translation. In PALMTREE, the "final" convolutions between the atoms $h^{*l}$ and $x^l$ adresses this issue.

% TODO add KSVD image
% TODO Malgouyres: talk about other models (???)

\section{PALMTREE algorithm}

\subsection{Dictionary-as-a-tree model \label{sec_tree_model}}
The current work, developed in \cite{chabiron_toward_2015} and \cite{chabiron_optimization_2016} propose a different way of structuring the matrix $D$. Instead of using "plain" atoms as columns and dealing with the matrix-vector product in $O(N^2)$, $D$ is defined by a tree structure $\mathcal{T}(\mathcal{V},\mathcal{E})$. 

The root $r$ and leaves $l$ vertex in $\mathcal{V}$ allow to define a branch as the successive edges growing from the root to one of the leaves.

Every edge $e$ in $\mathcal{E}$ bears a kernel $h^e$ and a support $s^e$. If the target image $y$ has its pixels in a "flattened" one-dimensional space $\mathcal{P}$, with
$$y : \mathcal{P} \rightarrow \mathbb{R}$$
then every kernel $h^e$ and support $s^e$ are also defined on $\mathcal{P}$ by $h^e:\mathcal{P} \rightarrow \mathbb{R}$ and $s^e:\mathcal{P} \rightarrow \{0,1\}$. The figure \ref{fig_example_kernel} exhibits an example of standard kernel filter on the left, and a $h^e$kernel on the right. The numerical values are the values of $h^e$ while the gray squares are the locations where $h^e$ has the right to have values on (i.e. the support $s^e$).

\begin{figure}[!ht]\centering
\includegraphics[width=0.8\textwidth]{figures/example-kernel.pdf}
\caption{(a) A well-known averaging kernel, which has fixed support ($3\times3$) and fixed values. (b) Example of a kernel $h^e$. The locations of the gray squares and the values (e.g. $.01$) actually change during PALMTREE. The shape of this $h^e$ (like a chessboard) has been shown experimentally to work well. \label{fig_example_kernel}}
\end{figure}

One column of the dictionary $D$ corresponds to the convolution of successive kernels on a given branch identified by the leaf $l$. We will denote that successive convolution by
\begin{flalign*}
&& H^{l} = & h^r * \dots * h^l  && \text{$\triangleright$ column/atom $l$ of $D$} \\
&&            = & h^{*l}
\end{flalign*}

Figure \ref{fig_matrix_vs_tree} outlines the relation between the tree and the matrix form. The dictionary can be written using the notation

$$D = \begin{bmatrix}C^5 C^6\end{bmatrix} \begin{bmatrix}C^1 & C^2 & 0 & 0 \\0 & 0 & C^3 & C^4\end{bmatrix}$$

with $C^i$ the block-circular matrix for $h^i$; it represents the final convolution $h^{*l}*x^l$ when using matrices.


\begin{figure}[!ht] \centering
	\includegraphics[width=0.8\textwidth]{figures/block-circular-matrix.pdf}
	\caption{This figure shows how to go from the tree defined by its kernels $(h^e)_{e \in \mathcal{E}}$ to a matrix $D$} \label{fig_block_circular}
\end{figure}

%\begin{figure}[!ht] \centering
%\includegraphics[width=0.9\textwidth]{figures/matrix-vs-tree.pdf} \caption{This figure shows how to go from the tree defined by its kernels $(h^e)_{e \in \mathcal{E}}$ to a matrix $D$. Note that this figure doesn't illustrates $Dx \approx y$ (standard matrix-vector product). Instead, it uses $D*x\approx y$ (convolutional product) which gives every possible translation for every atom.  \label{fig_matrix_vs_tree}}
%\end{figure}

Using the matrix notation, the dictionary-as-a-tree $D$ applied to a code $x$ approximates $y$:


$$Dx \approx y$$

The code $x$ belongs to $\mathbb{R}^{L\times N}$. $D$ has dimensions $N \times L$ and $y$ belongs to $\mathbb{R}^N$. 

Showing that $D$ can be written as a matrix allows to compare the performance of PALMTREE against a simple matrix-vector product. Here is the "PALMTREE way" of computing $Dx$:

\begin{align}
	Dx = \sum_{l \in \leaves} x^{l} * h^{*l}	 \label{eq_Dx_as_convolution}
\end{align}

where $h^{*l}$ denotes the successive convolution of every kernel on the branch that leads to leaf $l$.



\section{The (FTL) problem}

The dictionary update step is defined as 
\begin{align}
(FTL) \quad \underset{\substack{(h^\text{e})_{e}}}\min & \lVert Dx - y \rVert_2^2 \label{eq_ftl_energy}\\
\text{s.t. } & h^e_p = 0 \Rightarrow  s^e_p=0 \quad & \forall e \in \mathcal{E}, \forall p \in 1..N \label{eq_ftl_in_support} \\
 & \lVert h^e \rVert \le \gamma & \forall e \in \mathcal{E}\label{eq_ftl_kernel_finite_nrj}
\end{align}

The constraint \ref{eq_ftl_in_support} guarantees that each kernel $h^e$ only has non-zero values on its support $s^e$. \ref{eq_ftl_kernel_finite_nrj} constrains the overall energy of every support to be finite and prevents a specific kernel to "explode".

(FTL) can be rewritten as an unconstrained problem, shifting the constraint spaces $(D^e)_e$ to the objective function using the characteristic function $\chi_{D^e}$ defined for one edge $e$:
$$\chi_{D_e}(h^e) = \begin{cases} 0 &\text{ if } h^e \in D^e \\ +\infty & \ \text{otherwise}\end{cases}$$

(FTL) becomes:

\begin{align}
(FTL) \quad \underset{\substack{(h^\text{e})_{e}}}\min & \lVert D(x) - y \rVert_2^2 + \sum_{e}\chi_{D_e} (h^e)
\end{align}

\subsection{Interesting (FTL) properties}
$Dx$ can be seen (in \cref{eq_Dx_as_convolution}) as a polynomial of degree $2\text{depth}(\mathcal{T})$. This implies that it is possibly extremely non-convex, but it is infinitely differentiable.

$$\Phi((h^e)_{e \in \mathcal{E}}) = \lVert Dx-y \rVert^2_2$$

The interesting thing is that $\Phi$ is differentiable when only one $h^e$ varies. % TODO

% TODO \cref{proof_ftl_polynomial}) 
% TODO FTL is non-convex...

\subsection{PALMTREE, the algorithm for solving (FTL)}
% TODO explain PALMTREE

\section{Issues with PALMTREE algorithm and internship goals}
As explained in \cite[p. 23]{chabiron_optimization_2016}, the main drawbacks to use this convolutional tree model for practical dictionary learning (as many other applications) are the hand-made parts of the model, leading to arbitrary and sub-optimal results. Among them are the design of the tree (degree, depth...) and the choice of the supports. % TODO "degree" not understandable
\subsection{Choice of the tree}
The paper chooses to use a fixed tree structure, meaning that the tree (degree, depth) has been created \emph{ad-hoc} on a per-experiment basis, trying to mimic the frequency pyramid tiling of a curvelet decomposition. The number of leaves was also specifically chosen to match the number of atoms that was generated on the target image.

But getting an actual adaptative dictionary update step implies that designing the tree is made optimally and as part of the optimization process. This is one of the internship research axis.

\subsection{Choice of the kernel supports}

As described in \ref{sec_tree_model}, the supports $s^e$ are part of the dictionary-as-a-tree model. In \cite{chabiron_toward_2015}, the authors experimentally show that using fixed supports for approximating curvelets is possible. For that specific purpose, the results turned to be good. But the whole model was supposed to be able to approximate any kind of image, hence the need for "adaptative" supports instead of hand-made ones. Figure \ref{fig_example_kernel} shows in dashed-line squares the hand-made support. The gray square show what we would expect from an "adaptive" support. Instead of keeping the same shape for every target atom, the supports evolves and get closer to the target shape.

And as for the tree design, the choice of where are located the elements of each kernel support has been made (so far) arbitrarily. The support of the $h^e$ kernel of fig. \ref{fig_example_kernel} shows an example of typical layout chosen for the experiments in \cite{chabiron_optimization_2016}. 

But our goal is to be able to approximate any kind of atoms, as \ac{KSVD} is able to do. Being able to choose what elements should be in the kernel supports as part of the optimization process is another goal during this internship.

\begin{figure}[!ht] \centering
\includegraphics[width=0.6\textwidth]{figures/add-rm-elmts-support.pdf}
\caption{On the left, a simple curvelet atom which serves as target image. The right figure represents the locations of the support element for one of the kernels of the tree. The dashed squares represent the initial support given in figure \ref{fig_example_kernel}. The gray squares show the (supposedly) locations of the support elements after optimization. The elements have moved to the general direction of the curvelet.\label{fig_example_optimal_support}}
\end{figure}


\chapter{Experiments}

\section{First research axis: experimenting with adaptative supports}

After being able to correctly add and remove support elements, the second phase was to think about what elements we would add or remove, and when (i.e. when during the PALMTREE algorithm).

The figure \ref{fig_example_optimal_support} shows an example of what we think would happen when allowing to add and remove elements: the support would shape itself to better match the target atom (in fig.\ref{fig_example_optimal_support}, the atom shown is actually the whole target image).


\subsection{Adding an element}
Our first thought on how to add an element to a support was to find the best kernel among the tree edges, and then pick the best point(s) of that support to be added. We thought the gradient of the objective function would probably give that information. 

% TODO explain 

\subsection{Experiment 1: knowing what elements are worth adding}

This experiment was using a single-branch tree of depth 4 (see fig. \ref{fig_single_branch}). One of the four supports is picked for the whole experiment. This support will be called $s^e$.
\begin{figure}[!ht]\centering
\includegraphics[width=0.8\textwidth]{figures/xp-pts-worth-adding.pdf}
\caption{a} \label{fig_single_branch}
\end{figure}
First, a full minimization with many iterations and retries is made to give a common starting point. Then, for every (not already at 1) element $i \in \mathcal{P}$  of the support $s^e$, the element is added to the support, and the minimization continues (with no retries to get the same conditions for every added point).
The objective function of that minimization is then stored so that the location of $i$ is remembered. The element $i$ is then removed and we continue with $i+1$ % TODO write algo + formula


As a result, we get an image that represent, for each pixel, the minimized objective function if an element were added at that specific location. This extremely slow method allows us to compare with faster methods (like just computing the gradient).


\subsection{Experiment 2: use the gradient for choosing what should be added}
This experiment aims to simulate the choice of an atom by the OMP algorithm. As a short reminder, here is the step we are studying:
\begin{algorithm}[!ht] % TODO Finish algorithm
    \caption{MP (Matching Pursuit) algorithm for sparse approximation}
  \begin{algorithmic}[0]
    \Input signal $y \in \mathbb{R}^{n}$, dictionary $D \in \mathbb{R}^{n \times m}$ with $m>>n$, $x = 0_{\mathbb{R}^m}$
    \Output Code $x$ which is $k$-sparse
    \State \textbf{Initialization} $R^{(0)} = y$
    \While{$i \leq k$} \Comment note the fixed number of iterations
      \State $l =  \arg\max_{l = 1,\dots,l} |\left< d_l,R^{(i)} \right>|$ \label{alg_omp_pick_correlation}
        \Comment{find atom with max. correlation with $R$}
      \State $R^{(i+1)} = R^{(i)}-x_l d_l^{(i)}$
      \State $\hat{y} = \hat{y}+\langle R^{(i)}, d_{l}^{(i)} \rangle d_{l}^{(i)}$
      \State $i = i + 1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

At \cref{alg_omp_pick_correlation}, the algorithm chooses the column of the dictionary that matches the best the residual. This is where we thought this way of doing would serve our "add an element to one of the supports" algorithm.

% TODO define PALMTREE
Our goal is to, between two iterations of the PALMTREE algorithm, add a new element to one of the supports. That would lead to an optimization-driven construction of the kernels instead of designing them by hand.

But we have been questioning ourselves on the fairness of choosing an element to be added based the mere partial gradient. We remind that the \ref{alg_omp_pick_correlation} is actually computing a gradient:

\begin{align*}
l =& \argmax \left|\left< g_l,R^{(i)} \right>\right| \\
=& \argmax \left| D^TR^{(i)}\right| \\
=& \argmax \left| D^T(D\alpha - u)\right| \\
\end{align*}
So: does the partial gradient \ref{eq_partial_gradient} give 
\begin{align*}
\Phi((h^e)_{e \in \mathcal{E}}) = \left\| \sum_{l\in\leaves} x^l * h^{*l} \right\|^2
\end{align*}
\begin{align*} 
\nabla_{h^{e'}}\Phi((h^e)_{e \in \mathcal{E}}) = 2 \widetilde{H^{e'}} * (h^{e'}*H^{e'}-y^{e'})
\end{align*} \label{eq_partial_gradient}


\section{Does the gradient give enough information to choose an element to be added to the support?}

After being able to add
\subsection{Results}
\begin{figure}[!h]\centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_target.png}
\caption{Target}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_approx.png}
\caption{Approximation} \label{fig_simple_approx}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix.png}
\caption{Worth-adding matrix}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4.png}
\caption{Partial gradient}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix_bestvalues.png}
\caption{Best values of above figure}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4_bestvalues.png}
\caption{Best values of above figure}
\end{subfigure}
\caption{Minimization of a single branch tree with fixed supports. }
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.004407 & 7.9\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{2} edge. Note that for the "added point" RMSE, we took the minimum of all RMSE for every point possibly added. Here, the RMSE is at best increased by 7.9\%.}
\end{table}


Tests had been done solely on an already fully optimized tree, meaning that the "worth adding" matrix would be "close enough" to be close to the gradient...
But after checking with multiple trees (more or less optimized), here is the thing: the gradient seems to be always giving more or less the right "global" direction (\cref{fig_xp_grad_iterations})

\begin{figure}[!h]\centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix_bestvalues.png}
\caption{Worth-adding matrix}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
	\begin{subfigure}[b]{0.49\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_1thiteration_partgrad4_bestvalues.png}
	\caption{Grad. iter. 3}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_8thiteration_partgrad4_bestvalues.png}
	\caption{Grad. iter. 8}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_20thiteration_partgrad4_bestvalues.png}
	\caption{Grad. iter. 20}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4_bestvalues.png}
	\caption{Grad. iter. 200}
	\end{subfigure}
\end{subfigure}
\caption{Is the gradient giving the same information depending on the iteration? The conditions are the same as \cref{fig_simple_approx}. Only the best values are displayed.}
\end{figure}

\begin{figure}[!h]\centering
\begin{subfigure}[b]{0.99\textwidth}\centering
	\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_target.png}
		\caption{Target image}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_approx.png}
	\caption{Before adding}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_approx.png}
	\caption{After adding}
	\end{subfigure}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_objmatrix.png}
\caption{Worth-adding matrix} \label{fig_worth_adding_mat}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_partgrad4.png}
\caption{Gradient before} \label{fig_grad_before}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_partgrad4.png}
\caption{Gradient after} \label{fig_grad_after}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_objmatrix_bestvalues.png}
\caption{Best values of \ref{fig_worth_adding_mat}}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_partgrad4_bestvalues.png}
\caption{Best values of \ref{fig_grad_before}}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
\includegraphics[width=\textwidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_partgrad4_bestvalues.png}
\caption{Best values of \ref{fig_grad_after}}
\end{subfigure}
\caption{Effect of adding one point to the support $s^4$. The "before" tree is already minimized. The "after" tree has been added a point where the "worth-adding matrix" were minimal, at $(4,127)$.} \label{fig_before_after_adding}
\end{figure}

The experiment \ref{fig_before_after_adding} tries to show how is evolving an already optimized tree when adding the "best-to-be-added" point to the support, using the location of minimum of\ref{fig_worth_adding_mat}.



\begin{figure}[!h]\centering
\begin{subfigure}[b]{0.4\textwidth}\centering
	\begin{subfigure}[b]{1\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/variable_support/xp_128x128_sc2_angl1_K3_S3_node4_variable_support_target.png}
	\end{subfigure}
	\begin{subfigure}[b]{1\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/variable_support/xp_128x128_sc2_angl1_K3_S3_node4_variable_support_approx.png}
	\end{subfigure}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}\centering
\includegraphics[width=0.37\textwidth]{figures/variable_support/support.pdf}
\end{subfigure}

\caption{Minimization using a variable support. Starting from simple diracs, the best element (using greatest gradient) is added to one of the supports every 5 iterations, up to 36 elements (same number as the fixed support in \cref{fig_single_branch}).} \label{fig_variable_support}
\end{figure}
	
%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad4_bestvalues.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad4.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad3_bestvalues.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad3.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad2_bestvalues.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad2.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad1_bestvalues.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_partgrad1.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_target.png%xp_128x128_sc2_angl1_K3_S3_node4_variable_support_approx.png

\clearpage
\addcontentsline{toc}{chapter}{Appendix}
\appendix

\chapter{Some stuff I'm writing to remember things}

\section{Why is sparse coding used for denoising?}

The figure \ref{sparse_reduce_noise} shows a noisy signal $y$ living in a high-dimensional $N$ and defined by
$$y=\hat{y} + b$$
with $b$ following a centered Gaussian distribution $\mathcal{N}(0,\sigma^2I)$ and $\hat{y}$ the noise-less signal. We see that the distance $||b||$ is always lower than the projected distance.

This is because the deviation in the $N$ dimensional space can be written as
\begin{align*}
\sigma^2(b) =& \mathbb{E}\left[\lVert b-\mathbb{E}(b) \rVert^2 \right]\\
=& \mathbb{E}\left(\lVert b \rVert^2 \right)\\
=& ... \\ % TODO finish useless proof
=& \frac{1}{N}\lVert b \rVert^2
\end{align*}
which gives 
$$ \lVert b \rVert = \sigma\sqrt{N} $$
When projected to the $K$ dimensional space, the noise deviation becomes
$$\lVert b \rVert = \sigma\sqrt{K} $$
which is much better than the previous distance, provided that $K<<N$. 

\begin{figure}[!h]\centering
\includegraphics[width=0.5\textwidth]{figures/sparse-reduce-noise.pdf}
\caption{When projected onto a lower dimensional space, the standard derivation of the additive Gaussian noise $b \sim \mathcal{N}(0,\sigma^2)$ will be greatly reduced if $K<<N$. The sparser the representation the better the denoising. \label{sparse_reduce_noise}}
\end{figure}

\section{Why $FT(\widetilde{A}) = FT(A)^*$}
\begin{align*}
(\widehat{\widetilde{A}})_{m,n} =& \sum_{k=1}^M \sum_{l=1}^N \widehat{A}_{k,l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
=& \sum_{k=1}^M \sum_{l=1}^N A_{-k,-l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
\shortintertext{By changing variables $k'=-k$ and $l'=-l$, we get:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\shortintertext{And thanks to the $(M,N)$ periodicity of $A$, which means that $A_{i,j}=A_{i+kM,j+lN}$, $\forall (k,l) \in \mathbb{N}^2$, letting us with:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k'+M,l'+N} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
\shortintertext{With a second change of variables $k''=k'+M$ and $l''=l'+N$:}
=& \sum_{k''=-M+M}^{-1+M} \sum_{l''=-N+N}^{-1+N} A_{k'',l''} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
=& \sum_{k'=1}^{M} \sum_{l'=1}^{N} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\end{align*}
% TODO Proof not finished; this proof might be useless/overkill in my thesis...


\printbibliography

\end{document}



