\input{packages_config.tex} % <-- all the \usepackages
\input{mystyle.tex}
\input{acronyms.tex}
\input{macros.tex}
\author{Maël Valais}
\date{Updated on \today}
\title{Optimization of dictionaries structured in convolutional trees for sparse image representation - Master's Thesis}
\begin{document}
%\input{titlepage} % Uncomment to show title page

\chapter{Introduction}
\section{The need for sparse representations}
A sparse signal over some representation means that it can be written with as few information as possible – in other words, sparse means with many zeros. Many applications spanning from machine learning to image denoising heavily rely on the property that we can summarize – or more precisely approximate, or factorize – any signal using a proper representation. The job of transforming the raw\footnote{Any raw signal is actually represented by the canonical basis} signal to its sparse counterpart takes the form of an operator, often called dictionary or transform.

The Fourier transform is an ideal example of such operators. In figure \ref{explefourier}, the left picture $y$ has been represented using the Fourier basis, which can be written as a matrix $D$. The right picture shows the coefficients $x$ such that
$$x = D^Ty$$
\begin{figure}[!h]
\subcaptionbox{A picture featuring a blackboard at INSA Toulouse}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_spacial.pdf}}
 \subcaptionbox{Frequential representation of the blackboard using Fourier transform. Middle coefficients are coding for general and constant features of the picture while the corner values are coding for the details. The multiple white lines, spanning from the middle to the borders, outline the discontinuities caused by the edges of the blackboard.}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_frequen.pdf}}
  \caption{ ($Dx=y$) Example of decomposition of a signal $y$ using the dictionary $D$ made of Fourier series.} \label{explefourier}
\end{figure}
But Fourier is quite different to our convolutional tree structure. Note that Fourier representation is exact thanks to its analytic form. This allows fast implementations, meaning that the transform is fast (computing the above picture using \texttt{fft2} on matlab is extremely fast). Also, due to the fact that it is a basis (thus bijective), the inverse transform exists and is unique. % XXX useless? 

Also, we can see that the right picture doesn't seem to be sparse: many coefficients are non-zeros. Fourier is not adapted to every kind of signals, but can give good results in some cases.

Many other non-adaptative dictionaries exist, like the cosine transform (used by JPEG), the wavelet transform (used by JPEG2000, heavily used for encoding movies in theaters) or the curvelet transform. And as for the Fourier transform, these operators are known to be fast.

\section{Adaptative dictionaries}
As mentioned, analytically-based dictionaries do not always result to sparse representations, although allowing low computational time. Dictionary learning tries to address the adaptativeness issue. Instead of using basis matrices, we allow ourselves to use overcomplete dictionaries (more columns than lines) made of learned images (the atoms).
\begin{figure}[!h] \centering
\includegraphics[width=0.5\textwidth]{figures/sparsity-matrix.pdf}
\caption{Matrix view of $Dx$ when $D$ is overcomplete (much more columns – the atoms – than lines, which is the dimension of the signal $y$) \label{fig_overcomplete_matrix}}
\end{figure}
In figure \ref{fig_overcomplete_matrix}, the sparse code is multiplied to $D$ to give an approximation of $y$. Finding a good dictionary amounts to find the best atoms in $D$ such that every $y$ gives a good sparse representation – or code – $x$.

More formally, the dictionary learning problem is defined by
\begin{equation*}
\begin{aligned}
(DL) && \underset{D,x}{\min} \lambda \lVert x \rVert + \lVert Dx-y \rVert^2
\end{aligned}
\end{equation*}
\ac{KSVD} is a very well known algorithm for dictionary learning. Instead of having a pre-determined $D$, we are going to learn it.

As many of the concepts behind the PALMTREE algorithm developed in \cite{chabiron_optimization_2016} come from the K-SVD algorithm, the next section will introduce it.


\section{\ac{KSVD} algorithm}


K-SVD learns the dictionary by alternatively optimizing $x$ and $D$. Optimizing $x$ is the sparse coding phase, and optimizing $D$ is the dictionary update phase.


\begin{algorithm} % XXX Finish algorithm
    \caption{K-SVD (K-Singular Value Decomposition) algorithm for dictionary learning}
  \begin{algorithmic}[1]
    \Input learning signals $(y_i)_{i=1..N}$ forming columns of $Y \in \mathbb{R}^{n \times N}$
    \Output dictionary $D \in \mathbb{R}^{n \times K}$ with $K>>n$
    \State \textbf{Initialization} Initialize $D^{(0)}$
    \While{condition}
	\State \dots
    \EndWhile
  \end{algorithmic}
\end{algorithm}


\section{PALMTREE algorithm}

The current work, developed in \cite{chabiron_toward_2015} and \cite{chabiron_optimization_2016} gives a theoretical framework to the (FTL) problem. The (FTL) problem aims at approximating an ideal dictionary $D^{ideal}$ (which can be easily understood as a matrix) made of ideal atoms $a_i$ on its columns. The approximated dictionary will be called $D$ and the atoms formed by its columns $(H^f)_{f \in \leaves}$. 

Instead of using a standard matrix-vector product to compute $Dx$ and $D^Ty$, the operator $D(x)$ is defined as a convolutional tree:
$$D(x) = \sum_{f\in\leaves} H^f * x^f$$
The columns of $D$ – the atoms $H^f$ – are simply defined by the successive convolution of kernels $h^i$ from the root to the leaf
$$H^f = h^1 * h^2 \dots * h^k$$
% XXX code ?

To sum up the convolutional tree model, here are some outlines of the $D$ operator:
\begin{itemize}
\item a standard dictionary would require many translated versions of the same atom; here, the convolution $D*x$ does the job 
\item because of the convolution $D*x$, each "component" $x^f$ of the code $x$ is the same size as the target image $y$. We will talk about this block-wise code a bit farther
\end{itemize}


\begin{equation*} \begin{aligned}
Dx \approx y
\end{aligned} \end{equation*}


\begin{figure}[!h] \centering
\includegraphics[width=0.9\textwidth]{figures/matrix-vs-tree.pdf} \label{matrix_vs_tree}
\end{figure}



In this work, we will generally stay on the non-noisy case, leaving noisiness to further work.

Algorithm PALMTREE: a Gauss-Seidel algorithm (in the sense that we 

In dictionary learning, we call the columns of $D$ the \emph{atoms}.


This experiment aims to simulate the choice of an atom by the OMP algorithm. As a short reminder, here is the step we are studying:
\begin{algorithm} % XXX Finish algorithm
    \caption{MP (Matching Pursuit) algorithm for sparse approximation}
  \begin{algorithmic}[1]
    \Input signal $y \in \mathbb{R}^{n}$, dictionary $D \in \mathbb{R}^{n \times m}$ with $m>>n$, $x = 0_{\mathbb{R}^m}$
    \Output Code $x$ which is $k$-sparse
    \State \textbf{Initialization} $R^{(0)} = y$
    \While{$i \leq k$} \Comment note the fixed number of iterations
      \State $l =  \argmax_{l = 1,\dots,l} |\left< d_l,R^{(i)} \right>|$ \label{omp_pick_correlation}
        \Comment{find atom with max. correlation with $R$}
      \State $R^{(i+1)} = R^{(i)}-x_l d_l^{(i)}$
      \State $\hat{y} = \hat{y}+\langle R^{(i)}, d_{l}^{(i)} \rangle d_{l}^{(i)}$
      \State $i = i + 1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

At \cref{omp_pick_correlation}, the algorithm chooses the column of the dictionary that matches the best the residual. This is where we thought this way of doing would serve our "add an element to one of the supports" algorithm.

% XXX define PALMTREE
Our goal is to, between two iterations of the PALMTREE algorithm, add a new element to one of the supports. That would lead to an optimization-driven construction of the kernels instead of designing them by hand.

But we have been questioning ourselves on the fairness of choosing an element to be added based the mere partial gradient. We remind that the \ref{omp_pick_correlation} is actually computing a gradient:

\begin{align*}
l =& \argmax \left|\left< g_l,R^{(i)} \right>\right| \\
=& \argmax \left| D^TR^{(i)}\right| \\
=& \argmax \left| D^T(D\alpha - u)\right| \\
\end{align*}
So: does the partial gradient \ref{eq_partial_gradient} give 
\begin{align*}
\Phi((h^e)_{e \in \edges}) = \left\| \sum_{f\in\leaves} \code^f * h^{\CC(r,f)} \right\|^2
\end{align*}
\begin{align*} 
\nabla_{h^{e'}}\Phi((h^e)_{e \in \edges}) = 2 \widetilde{H^{e'}} * (h^{e'}*H^{e'}-y^{e'})
\end{align*} \label{eq_partial_gradient}

\section{Issues with PALMTREE algorithm and internship goals}
As explained in \cite[p. 23]{chabiron_optimization_2016}, the main drawbacks to use this convolutional tree model for practical dictionary learning (as many other applications) are because some aspects of the model are hand-made, leading to arbitrary and sub-optimal results.
\subsection{Choice of the tree}
The paper chooses to use a fixed tree structure, meaning that the tree (degree per node, depth) has been created \emph{ad-hoc} on a per-experiment basis, trying to mimic the frequency pyramid tiling of a curvelet decomposition. The number of leaves was also specifically chosen to match the number of atoms that was generated on the target image.

But getting an actual adaptative dictionary update step implies that designing the tree is made optimally and as part of the optimization process. This is one of the internship research axis.

\subsection{Choice of the kernel supports}
The kernels $h^e$ mentioned above are similar to traditional kernel filters used in signal processing, like the well-known averaging filter (fig. \ref{fig_example_kernel}. The two slight differences are that, for one, that instead of having fixed values (like $1/9$ for a $3\times3$ averaging filter), the values can be changed (those are actually the variables to be optimized); for two, the supports (the locations of the points of the kernel) are not fixed (the fig. \ref{fig_example_kernel} shows a support with 9 elements but with spaces between those elements).


\begin{figure}[!ht]\centering
\includegraphics[width=0.5\textwidth]{figures/example-kernel.pdf}
\caption{Comparison of the well-known averaging kernel, which has fixed support ($3\times3$) and fixed values, and the kernels used in the PALMTREE, which does not have fixed values or support (this $5\times5$ shape will be explained later). You can imagine that the value of this $h^e$ kernel are the result of the PALMTREE algorithm.\label{fig_example_kernel}}
\end{figure}
More formally, the support of the kernel $h^e$is defined as the element (i.e. the points) of $h^e$ that are free to be non-zero.

And as for the tree design, the choice of where are located the elements of each kernel support has been made (so far) arbitrarily. The support of the $h^e$ kernel of fig. \ref{fig_example_kernel} shows an example of typical layout chosen for the experiments in \cite{chabiron_optimization_2016}. 

The reason why the previous work only consider approximating (mainly) curvelet atoms is because \cite{chabiron_apprentissage_2015} has shown that they can be well approximated using chain convoluted fixed-support kernels.

But our goal is to be able to approximate any kind of atoms, as \ac{KSVD} is able to do. Being able to choose what elements should be in the kernel supports as part of the optimization process is another goal during this internship.

\begin{figure}[!ht] \centering
\includegraphics[width=0.7\textwidth]{figures/example-optimized-support.pdf}
\caption{On the left, a simple curvelet atom which serves as our target image. The right figure represents the positions of the support element for one of the kernels of the tree. The dashed squares represent the initial "fixed" support given in figure \ref{fig_example_kernel}. The gray squares show the positions of the support elements after having optimized: the elements have moved to the general direction of the curvelet.\label{fig_example_optimal_support}}
\end{figure}

Figure \ref{fig_example_optimal_support} shows what we could expect after adding and removing elements from one of the kernel supports: the kernel will "melt" to match the shape of the target image.



\section{Experimenting with adaptative support}

After being able to correctly add and remove support elements, the second phase was to think about what elements we would add or remove, and when (i.e. when during the PALMTREE algorithm).

\subsection{}

Our first thoughts on how to 



\section{Does the gradient give enough information to choose an element to be added to the support?}

After being able to add
\subsection{Results}
\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_gradient_node_2.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.004407 & 7.9\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{2} edge. Note that for the "added point" RMSE, we took the minimum of all RMSE for every point possibly added. Here, the RMSE is at best increased by 7.9\%.}
\end{table}


% XXX Proof not finished; this proof might be useless/overkill in my thesis...
\begin{align*}
(\widehat{\widetilde{A}})_{m,n} =& \sum_{k=1}^M \sum_{l=1}^N \widehat{A}_{k,l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
=& \sum_{k=1}^M \sum_{l=1}^N A_{-k,-l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
\shortintertext{By changing variables $k'=-k$ and $l'=-l$, we get:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\shortintertext{And thanks to the $(M,N)$ periodicity of $A$, which means that $A_{i,j}=A_{i+kM,j+lN}$, $\forall (k,l) \in \mathbb{N}^2$, letting us with:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k'+M,l'+N} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
\shortintertext{With a second change of variables $k''=k'+M$ and $l''=l'+N$:}
=& \sum_{k''=-M+M}^{-1+M} \sum_{l''=-N+N}^{-1+N} A_{k'',l''} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
=& \sum_{k'=1}^{M} \sum_{l'=1}^{N} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\end{align*}

\section{Why is sparse coding used in image recovery?}

The figure \ref{sparse_reduce_noise} shows a noisy signal $y$ living in a high-dimensional $N$ and defined by
$$y=\hat{y} + b$$
with $b$ following a centered Gaussian distribution $\mathcal{N}(0,\sigma^2I)$ and $\hat{y}$ the noise-less signal. We see that the distance $||b||$ is always lower than the projected distance.

This is because the deviation in the $N$ dimensional space can be written as
\begin{align*}
\sigma^2(b) =& \mathbb{E}\left[\lVert b-\mathbb{E}(b) \rVert^2 \right]\\
=& \mathbb{E}\left(\lVert b \rVert^2 \right)\\
=& ???\\
=& \frac{1}{N}\lVert b \rVert^2
\end{align*}
which gives 
$$ \lVert b \rVert = \sigma\sqrt{N} $$
When projected to the $K$ dimensional space, the noise deviation becomes
$$\lVert b \rVert = \sigma\sqrt{K} $$
which is much better than the previous distance, provided that $K<<N$. 

\begin{figure}[!h]\centering
\includegraphics[width=0.5\textwidth]{figures/sparse-reduce-noise.pdf}
\caption{When projected onto a lower dimensional space, the standard derivation of the additive Gaussian noise $b \sim \mathcal{N}(0,\sigma^2)$ will be greatly reduced if $K<<N$. The sparser the representation the better the denoising. \label{sparse_reduce_noise}}
\end{figure}



\printbibliography

\end{document}



