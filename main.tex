\input{packages.tex}  % <-- all the \usepackages
\input{pagestyle.tex} % <-- \pagestyle{body}
\input{macros.tex}    % <-- \x, \y, \D...
\input{acronyms.tex}  % <-- acronyms (\ac{PALM}...)
\author{Maël Valais}
\date{Updated on \today}
\title{Optimization of dictionaries structured in convolutional trees for sparse image representation - Master’s Thesis}
\begin{document}
\input{titlepage} % Uncomment to show title page
\pagestyle{empty} \restoregeometry
%\cleardoublepage % White until the next even page
\pagestyle{body}
{\let\clearpage\relax \abstract{}} \todo[inline]{Write an abstract}
{\let\clearpage\relax \centering \subsection*{Foreword}} \addcontentsline{toc}{subsection}{Foreword}\todo[inline]{Write a foreword}
\tableofcontents
%{\let\clearpage\relax\listoffigures}
%{\let\clearpage\relax\listoftables}
%{\let\clearpage\relax\listofalgorithms}



\chapter{Introduction} %\markboth{Introduction}{}}

In this chapter, we introduce and review the main existing dictionaries that have been used for sparse representations, from non-adaptative transforms to learned dictionaries via the intermediate over-complete dictionaries. The second part of this chapter provides the general motivations and objectives of this work as well as the related works.

\section{The need for sparse representations}

A sparse signal over some representation means that it can be expressed using only a few elements of the representation – in other words, sparse means with many zeros. Many applications ranging from machine learning to image denoising and image recognition heavily rely on the property that we can summarize (or more exactly approximate) any signal using a proper sparse representation. The job of obtaining the raw\footnote{A “raw” signal is represented in the canonical basis} signal from its sparse counterpart can be written in terms of an operator, often called \emph{dictionary} or \emph{transform}.

\section{Basis and redundant dictionaries}

The \ac{DFT} is a classical example of an operator used for sparsity. The \ac{DFT} can be written as a matrix $\D$, where $\D$ denotes the dictionary. Finding the Fourier representation – which we will refer as the \emph{code} $\x$ – of an image $\y$ amounts to compute 
\begin{equation*}\x = \D^T\y.\end{equation*}

\begin{figure}[!ht]
\subcaptionbox{Picture $\y$ with many discontinuities.}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/fourier/image.pdf}}
 \subcaptionbox{Result of applying the Fourier transform to $\y$}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/fourier/fourier.pdf}}
  \caption{Decomposition $\D^T\y=\x$ of a signal $\y$ using the dictionary $\D$ made of Fourier series. In $\x$, the middle coefficients are coding “large” features while the corner values are coding the details. The multiple white lines, ranging from the middle to the borders, outline the discontinuities caused by the blackboard edges; these coefficients are scattered (hence not sparse), which shows that the Fourier basis has representation problems in presence of discontinuities.} \label{fig_fourier}
\end{figure}

However, the Fourier transform is quite different from the representations we have in mind, namely the convolutional tree structures. First, Fourier coefficients form a basis, implying that it is a one-to-one representation in the sense that every code $\x$ is associated with a unique image $\y$. This basis is also orthogonal, meaning that the operator is stable (it does not amplify errors), as well as normed (each element has a norm equal to 1).

More importantly, the \ac{DFT} has interesting properties that allow fast implementations. For example, applying the Matlab function \texttt{fft2} to the \cref{fig_fourier} takes less than a millisecond. If we assume $N$ to be the dimension of the concatenated\footnotemark[2] image, instead of computing a time-expensive $O(N^2)$ matrix-vector product, the fast Fourier transform only requires a computational cost of the order $O(N \log N)$.

\footnotetext[2]{Also referred as “vectorized”; we consider 2-D images as one-dimensional vectors. A $16 \times 16$ image would give a vector of dimension 256.}


The Fourier transform is widely used as an operator for sparse representations, mainly when dealing with smooth signals that can be approximated by a sum of sinusoids. However, this operator does not generally perform well on images since they contain many discontinuities that are far from being representable using sinusoids.

The problem with discontinuities is illustrated in \cref{fig_fourier}. As an example, the bottom edge of the blackboard (which is a strong discontinuity) induces a long vertical trail of high coefficients in $\x$. Because of discontinuities, $\x$ is forced to have many non-zero coefficients. 

Along with Fourier, many other basis dictionaries exist; we can mention the cosine transform which is at the core of JPEG compression. We can also cite the wavelet transform, used in JPEG2000 compression. Note that both wavelets and cosine are stable and have fast implementations.

\section{Redundant dictionaries}

Basis dictionaries were generalized by using multiple bases to form a stable and redundant (also called over-complete) dictionary in \cite{shaobing_chen_atomic_2001}. For example, combining a canonical basis with a cosine basis allows both peaks and sinusoids to be represented.

Other redundant dictionaries, like the curvelet transform, do not have the useful properties of bases, yet being stable and having a fast implementation. It is interesting to note that contrary to Fourier or cosine transforms, the curvelets can code (to an extent) non-smooth signals.

% Notes for myself
% 1. tight frame = preserves the norm
% 2. orthogonality preserves the norm (norm doesn’t make the errors explode)
% 3. orthogonal means that it is stable

Although offering excellent performances for sparse representations thanks to their fast implementations, redundant dictionaries have the drawback to lack adaptativity.


\section{Learned dictionaries}
Previously mentioned dictionaries are said to be “non-adaptative”, meaning that they are not able to sparsely code every possible type of signal. Dictionary learning allows us to create “adaptative” dictionaries. Instead of being fixed, the atoms of an “adaptative” dictionary are based on a learning set of example images, denoted by
\begin{equation*}\Y = \begin{bmatrix} \y_1 & \dots & \y_S \end{bmatrix}\end{equation*}
where the columns of $\Y$ are the vectorized\footnote{A vectorized matrix of dimension $R \times C$ of an image is the concatenation of all columns to form a vector of size $R \cdot C$} images $\y_i \in \R^N$, and $S$ the number of sample images. The dictionary $\D$ is made of $K$ columns $\d_i$ (the atoms) such that the number of atoms is much larger than the dimension of a single image $\y_i$ ($K \gg N$):
\begin{equation*}\D = \begin{bmatrix} \d_1 & \dots & \d_K \end{bmatrix}.\end{equation*}
The codes $\X$ are defined by
\begin{equation*}\X = \begin{bmatrix} \x_1 & \dots & \x_{S} \end{bmatrix}\end{equation*}
where $\X$ is the concatenation of $S$ column vectors $\x_i$ (the codes) of dimension $K$. The dictionary learning problem is defined as
\begin{align}
\underset{\D,\X}{\min}~ & \lVert \X \rVert_1 + \lambda\lVert \D\X-\Y \rVert^2_F \tag{$DL$} \label{eq_dl} \\
\text{s.t.}~ & \lVert \d_k \rVert \le \gamma & \forall k = 1,\dots,K\label{eq_dl_finite_norm}
\end{align}
where $\lVert . \rVert_F$ denotes the Frobenius norm and $\lVert . \rVert_1$ is the $l_1$ norm defined by $\|\X_i\|_1  = \sum_{i=1}^S  sum_{j=1}^K \|\x_i^j \|$.


$\lVert \X \rVert_1 = \sum_{i=1}^S |\x_i|$. Note that the constraint \ref{eq_dl_finite_norm} prevents the atoms from having an arbitrarily large norm.

The problem \eqref{eq_dl} can be understood as finding the atoms of $\D$ that give the sparsest (with as many zeros as possible) representation $\x_i$ approximating $\y_i$ for all $i = 1,\dots,S$. This approximation is denoted as
\begin{equation*}\D\X \approx \Y.\end{equation*}

\Cref{fig_overcomplete_matrix} gives an idea of what we mean by approximating an image $\y$ using the matrix-vector product of a sparse code $\x$ and a learned dictionary $\D$. For this example, $\y$ is represented by a linear combination of three atoms of $\D$: $\x$ is 3-sparse.

\begin{figure}[!ht] \centering
\includegraphics[width=0.90\textwidth]{figures/sparsity-matrix.pdf}
\caption{Matrix view of $\D\x$ when $\D$ is over-complete (much more columns than lines).}\label{fig_overcomplete_matrix}
\end{figure}


The non-linearity caused by the product $\D\X$ induces a non-convex objective function, making it difficult to optimize simultaneously with respect to $\D$ and $\X$. To work around this problem, many algorithms solve \eqref{eq_dl} by optimizing alternatively with respect to the dictionary $\D$ and the codes $\X$.

\subsection{Existing dictionary learning algorithms}
Among the many available algorithms (and their multiple versions) for dictionary learning, the \ac{KSVD} algorithm is a typical example. Introduced in \cite{aharon_k-svd:_2006} and inspired by the widely known \gls{KMeans} algorithm, it is used in many image processing applications. This algorithm is detailed in the next section.

Another approach, more scalable, is denoted as “online” dictionary learning. The Online Dictionary Learning algorithm, proposed in \cite{mairal_online_2010}, learns one image after the other instead of learning the whole set of images simultaneously. Thus it is very suitable for huge sets of learning images.

For a thorough review of every transform available (developed before 2010), the reader is invited to take a look at  \cite{rubinstein_dictionaries_2010}.


\subsection{Example with the \acs{KSVD} algorithm}

Among the many existing alternating algorithms for dictionary learning, the \ac{KSVD} algorithm is well known for its state-of-the-art performance in image denoising. \ac{KSVD} is responsible of many concepts behind the \acs{PALMTREE} algorithm (the algorithm we are trying to improve and developed in \cite{chabiron_optimization_2016}). This section summarizes the key elements of the \ac{KSVD} algorithm.

\ac{KSVD} does not actually solve \eqref{eq_dl}; instead, it gives a solution for a “stronger” problem in the sense that the level of sparsity in \ac{KSVD} is enforced by $\gamma$, leading to the following problem
\begin{align*}
\underset{\D,\X}{\min}~ & \lVert \D\X-\Y \rVert^2_F \tag{$DL_2$}\label{eq_dl_ksvd} \\
s.t.~& \lVert \x_i \rVert_0 \le \gamma & \forall i \in 1,\dots,S
\end{align*}
where $\lVert . \rVert_F$ denotes the Frobenius norm and $\lVert . \rVert_0$ is the pseudo-norm $l_0$ (also known as “counting function”).

The \emph{Sparse Coding step} optimizes the cost function with respect to $\x$; this step basically uses \ac{OMP} (detailed in \cref{alg_omp}) or any other pursuit algorithm. The \emph{Dictionary Update step} optimizes the cost function w.r.t. $\D$. This is the step that was studied in \cite{chabiron_optimization_2016}. Note that the full \ac{KSVD} algorithm is presented in \cref{sec_ksvd_detail}, \cpageref{sec_ksvd_detail}.

\Cref{fig_ksvd} gives an example of what “learning $\D$” with the \ac{KSVD} algorithm means. As the image (a) is too big ($512 \times 512$) to be learned directly, we must chop it into many small patches, e.g. of size $16 \times 16$. \Cref{fig_ksvd_patches} shows some of these patches contained in $\Y$, which actually holds 10240 patches ($\Y$ has dimension $128 \times 10240$). The dictionary $\D$ of size $128 \times 128$ (it has 128 atoms) is shown in (c). Note that $\D$ has been learned with a sparsity of 10 and that the process took about five minutes (stopped after 20 iterations).
\noindent The dictionary in \cref{fig_ksvd_dict} can now be used for denoising the noisy image of \cref{fig_ksvd_image}. If we would like a dictionary for image recognition or compression, we would probably need many additional images in order to obtain a good classification or compression performance.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.40\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/ksvd/tableau_512x512.png}
	\caption{Image used for learning} \label{fig_ksvd_image}
\end{subfigure}
\begin{subfigure}[b]{0.29\textwidth}\centering
	\includegraphics[width=0.7\textwidth]{figures/ksvd/patches.pdf}
	\caption{Image patches $\Y$} \label{fig_ksvd_patches}
\end{subfigure}
\begin{subfigure}[b]{0.29\textwidth}\centering
	\includegraphics[width=0.7\textwidth]{figures/ksvd/dictionary.pdf}
	\caption{Learned $\D$}\label{fig_ksvd_dict}
\end{subfigure}
\caption{Learning $\D$ on image patches using \ac{KSVD}.}\label{fig_ksvd}
\end{figure}


\subsection{Problem of atom scalability: multi-scale dictionaries}
As the learned \ac{KSVD} dictionary relies on the $O(N \cdot K)$ matrix-vector product and has to be over-complete ($K \gg N$) in order to achieve sparsity, we easily understand that $N$ cannot take an arbitrarily large value. In practice, the example images (also called patches) and atoms of the dictionary are generally of size at most $16 \times 16$.

Dictionaries with bigger atoms are known as multi-resolution (also called multi-scale), meaning that the atoms can contain either details or large features computed from the images.


\subsection{Problem of atom redundancy: convolutional dictionaries} \label{sec_atoms_redund}
The \ac{KSVD} algorithm is not translation-invariant in the sense that multiple atoms of the dictionary can have the same detail but slightly translated. The authors of \cite{mailhe_shift-invariant_2008} worked on making the update step invariant by translation, but it still needs a lot of tuning and finding the right parameters.

Translation-invariant transforms are known as convolutional dictionaries. Instead of many atoms containing translated versions of the same feature, convolutional dictionaries contain all possible translations for every atom; contrary to traditional learned dictionaries, it is unlikely to find two atoms containing the same detail with different locations.

Many state-of-the-art machine learning techniques are based on trees of convolutions, such as the Convolutional Neural Networks used in Deep learning \cite{lecun_deep_2015}.

\section{Research motivations and objectives}
Sparse representations have been used successfully in many applications. The use of these representations is only limited by the performance of their transforms. By performance, we mean that the transform must be
\begin{itemize}
\item[--] fast (fast computation of $\D\x$),
\item[--] adaptative (must be learned on a set of images)
\item[--] and stable (does not amplify errors).
\end{itemize}

As mentioned, many algorithms are available in the literature. However, they are often fast but not adaptative (such as the Fourier transform) or adaptative and not fast (such as \ac{KSVD}).

The main objectives of this work is to develop a multi-resolution  dictionary model (allowing large atoms to be considered) that is fast and adaptive, based on the principle of convolutional trees. During his PhD thesis, Olivier Chabiron has been working on a \Gls{treemodel}, presented in the paper \citetitle{chabiron_optimization_2016} (\cite{chabiron_optimization_2016}). Our work will focus on this model for \eqref{eq_dl}.


\section{Related work}
Two other teams are currently actively working on multi-resolution transforms:
\begin{itemize}
	\item[--] the team lead by Mickeal Elad proposed a dictionary model based on wavelets in \cite{sulam_trainlets:_2016};
	\item[--] Rémi Gribonval’s team has proposed a factorization-based dictionary model in  \cite{le_magoarou_flexible_2016}.
\end{itemize}
Their work have been published very recently (in early 2016); this master's thesis does not consider the approaches investigated in these papers. However, studying these advances will conducted just after the end of this internship during a PhD thesis.


\chapter{State of the art}
This chapter introduces the \Gls{treemodel} as well as its associated dictionary learning problem, \acs{FTL}, and the algorithm \ac{PALMTREE} that solves this problem. The end of this chapter is dedicated to detailing the specific objectives of this internship.

\section{Dictionary structured by the \Gls{treemodel}} \label{sec_tree_model}
The work studied in \cite{chabiron_toward_2015} and \cite{chabiron_optimization_2016} offers a different way of structuring the matrix $\D$, namely the \emph{\Gls{treemodel}}. Instead of using “plain” atoms as columns and dealing with the matrix-vector product with complexity $O(N \cdot K)$, $\D$ is defined by a convolutional tree structure \begin{equation*}\T(\V,\E)\end{equation*} where each edge $e$ of $\E$ is characterized by a kernel $\h^e$ and a support $\s^e$. The leaves $l \in \L \subset \V$ associated with the root $r \in \V$ allow us to define a branch as the successive edges growing from the root (top) to one of the leaves (bottom).

\begin{figure}[!ht]\centering
\includegraphics[width=\textwidth]{figures/tree.pdf}
\caption{View of a convolutional tree; the $\h^e$ and $\s^e$ are what define $\D$}\label{fig_tree}
\end{figure}

The target image $\y$ as well as the kernels $\h^e$ and supports $\s^e$ are vectors of dimension $N$. The supports $\s^e$ only take the values 0 (corresponding value in $\h^e$ must be 0) or 1 (corresponding value in $\h^e$ can be non-zero). For easier reading, we will denote the families $(\h^e)_{e \in \E}$ and $(\s^e)_{e \in \E}$ by \begin{equation*}\h = (\h^e)_{e \in \E}\end{equation*} and \begin{equation*}\s = (\s^e)_{e \in \E}.\end{equation*}

The code $\x$ is a “big” vector of dimension $K$ with $K = N \cdot |\L|$ such that \begin{equation*}\x = \begin{bmatrix}x_{11} \\ \vdots \\ x_{|\L|N}\end{bmatrix} \quad \text{with $x_{lp}$ the coefficient of $\x$ at leaf $l$, point $p$.}\end{equation*} 
Finally, the dictionary matrix associated\footnote{The Convolutional tree dictionary can be rewritten into a matrix; see \cref{sec_matrix_vs_tree}} with the \gls{treemodel} is denoted $\D$ and is of dimension $N \times K$, and the image $\y$ is a vector of dimension $N$.

In the following, we will denote the image space using the notation 
\begin{equation*}\P = \{p ~|~ p=1,\dots,N\}\end{equation*}
and the image $\y$ will be defined as
\begin{equation*}\y = \begin{bmatrix}y_1 \\ \vdots \\ y_N\end{bmatrix} \quad \text{with $y_p$ the $p$-th coefficient of $\y$.}
\end{equation*}

For more convenience, the images $\h^e$, $\x^l$ and $\y$ are defined as “circular” or “periodic” signals, meaning that they are defined for every $p \in \mathcal{Z}$. With that in mind, we can define the circular convolution for two circular signals $\h$ and $\h’$ of $\R^N$ as
\begin{equation*}(\h * \h’)_p = \sum_{p’ \in \P} \h_{p-p’} \h’_{p’}\end{equation*}

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.20\textwidth}\centering
\includegraphics[width=0.90\textwidth]{figures/kernel-exple.pdf}
\caption{Averaging kernel: fixed support and values}
\end{subfigure}
\begin{subfigure}[b]{0.79\textwidth}\centering
\includegraphics[width=0.90\textwidth]{figures/kernel-h_e.pdf}
\caption{Kernel $\h^e$ (left) and its associated support $\s^e$ (right).}\label{fig_example_kernel-h_e}
\end{subfigure}
\caption{Two examples of convolutional kernels.}\label{fig_example_kernel}
\end{figure}

\Cref{fig_example_kernel-h_e} displays examples of a standard kernel (left) and a kernel $\h^e$ (middle) associated with its support $\s^e$ (right). The gray squares represent the “ones” of the support while the kernel is represented by values inside the squares.

We denote the convolution of successive kernels on a given branch identified by the leaf $l$ by
\begin{equation*}\h^{*l} = \h^r * \dots * \h^l.\end{equation*}

Instead of using the standard matrix-vector product, the convolutional tree dictionary computes $\D\x$ as follows
\begin{align}
	\D\x = \sum_{l \in \L} \x^l * \h^{*l}. \label{eq_Dx_as_conv}
\end{align}
Moreover, the convolutional tree dictionary $\D$ applied to a code $\x$ approximates $\y$ as follows
\begin{equation*}\D\x \approx \y. \end{equation*}

\section{The \acs{FTL} problem}

The dictionary update step associated with the \gls{treemodel} is defined as
\begin{align}
\underset{\substack{(\h^e)_{e}}}\min ~ & \lVert \D\x - \y \rVert_2^2 \tag{$FTL$} \label{eq_ftl}\\
\text{s.t.~} & \s^e_p=0 \Rightarrow \h^e_p = 0 \quad & \forall p=1,\dots,N ,~\forall e \in \E \label{eq_ftl_in_support} \\
 & \lVert \h^e \rVert \le \gamma & \forall e \in \E\label{eq_ftl_kernel_finite_nrj}
\end{align} 
The constraint in (\ref{eq_ftl_in_support}) guarantees that each kernel $\h^e$ has its non-zero values in its support $\s^e$. The constraint (\ref{eq_ftl_kernel_finite_nrj}) ensures that the overall energy of every support is finite and prevents a specific kernel to “explode” (similar to the constraint \eqref{eq_dl_finite_norm} of the \eqref{eq_dl} problem).

\ac{FTL} can be rewritten as an unconstrained problem by introducing a characteristic function $\chi_{\Dspace^e}$ defined for each edge $e$ as follows

\begin{align*}
	\chi_{\Dspace^e}(\h^e) = \begin{cases} 0 &\text{ if } \h^e \in \Dspace^e \\ +\infty & \ \text{otherwise}\end{cases} & \quad \text{with} \quad \
		\Dspace^e = \begin{Bmatrix}  \h^e_p ~|~ \h^e_p=0 ~\text{when}~ \s^e_p=0 \quad \forall p=1,\dots,N\\ \text{and }\lVert \h^e \rVert \le \gamma \end{Bmatrix}
\end{align*}
The unconstrained problem is
\begin{align}
\underset{\substack{(\h^e)_{e}}}\min ~ & \lVert \D\x - \y \rVert_2^2 + \sum_{e}\chi_{D_e} (\h^e). \tag{${FTL}_2$} \label{eq_ftl2}
\end{align}
For later references, the objective function of \ac{FTL} will be denoted as
\begin{align}
\Phi[(\h^e)_{e \in \E}] = \lVert \D\x-\y \rVert^2_2
\end{align}

\subsubsection{Interesting properties of \ac{FTL}}

The convolution of sparse kernels in \ac{FTL} leads to theoretically highly performant dictionaries. The computational cost of $\D\x$ using \cref{eq_Dx_as_conv} is reduced to $O(Q \cdot N)$ ($Q$ is the number of elements in all supports, i.e. $Q=|\{s^e_p ~|~ s^e_p = 1, \forall (e,p) \in \E \times \P\}|$). And because $Q$ only depends on the number of edges and is designed to be significantly smaller than  $N$, using the \gls{treemodel} decreases greatly the cost of using the dictionary and is comparable to the Fourier transform performance (in $O(N \log N)$).

\begin{table}[!ht] \centering 
\caption{Computational cost of $\D\x$} \label{table_comparison_Dx_costs}
\begin{tabular}{c|c}
Using matrix-vector product & Using \gls{treemodel} \\\\ \hline \\
$O(K \cdot N)$ & $O(Q \cdot N)$
\end{tabular}
\end{table}


However, the product $\D\x$ can be seen as a multivariable polynomial of degree the depth of $\T$ (variables are $(\h^e)_{e \in \E}$ and $\x$). This non-linearity makes the objective function possibly strongly non-convex, suggesting that we would end up trapped by its many suboptimal critical points of $\Phi$.

And yet, the authors of \cite{chabiron_optimization_2016} have successfully proved that this problem is actually tractable. More surprisingly, they discovered that optimizing \ac{FTL} is possible and that the solutions given by \acs{PALMTREE} are encouraging.


\section{The PALMTREE algorithm}\label{sec_palmtree}

The authors of \cite{chabiron_optimization_2016} have shown that \eqref{eq_ftl2} can be solved using the \ac{PALM} algorithm proposed in \cite{bolte_proximal_2014}. The \ac{PALM} algorithm does a proximal gradient iteration successively on the different blocks of variables; \cite{bolte_proximal_2014} provides a proof of convergence towards a critical point for non-convex problems. In our case, one block corresponds to one kernel $\h^e$.


regularizes the non-convex objective function using a simple linear approximation with respect to one block of variables. One block corresponds to one kernel $\h^e$.

The implementation of the generic \ac{PALM} algorithm for solving the \eqref{eq_ftl2} problem has been named \ac{PALMTREE}. 

An outline of the algorithm is given in \cref{alg_palmtree}, where $t^e$ denotes the step size and $\text{prox}^f_\gamma$ is the proximal operator for prior function $f$ defined by \begin{equation*}\text{prox}^f_\gamma(y) = \underset{x}{\argmin}~ f(x) + \frac{\gamma}{2} \lVert x - y \rVert^2_2\end{equation*} which, for $f = \chi_{\Dspace^e}$, is easily computed by a projection onto the $\Dspace^e$.

Details on the step size $t^e$ (which requires the computation of a Lipschitz constant) and the proximal operator $\text{prox}^{\chi_{\Dspace^e}}_{t^e}$ are given in \cite{chabiron_optimization_2016}. We will focus on the way the gradient is computed to get more insight from \ac{PALMTREE}.

We define $\H^e$ as the convolution of every branches that crosses the edge $e$ except for the kernel $\h^e$, i.e. \begin{equation*}\H^e = \h^r * \dots * \h^{\text{above}(e)} * \bcancel{\h^{e}} * \sum_{l \in \text{leaves(e)}} \h^{\text{below}(e)^l} * \dots * \h^{l}\end{equation*} where $\text{leaves}(e)$ denotes the leaves that can be reached through $e$, $\text{above}(e)$  the edge right above $e$ and $\text{below}(e)^l$ the edge below $e$ that leads to the leaf $l$.

Using this notation, the authors of \cite{chabiron_optimization_2016} have shown that the partial gradient of $\Phi$ w.r.t. $\h^e$ is 
\begin{equation*}\nabla_{\h^e} \Phi[(\h^f)_{f \in \E}] = 2 \H^e * \Res\end{equation*} with $\Res$ the residual denoted by
\begin{equation*}\Res = \D\x - \y\end{equation*}

Because of the projection onto the $\Dspace^e$ set, it is sufficient to compute the partial gradient at the support locations. As explained further in \cref{sec_full_grad}, this means that the convolution $2 \H^e * \Res$ can be efficiently computed using a simple convolution. However, computing the “full” gradient (meaning on every point of kernel $\h^e$) cannot be computed efficiently this way.

\begin{algorithm}[!ht]
    \caption{\ac{PALMTREE} (Proximal Alternating Linearized Minimization for \Gls{treemodel}) algorithm for Dictionary Update}\label{alg_palmtree}
  \begin{algorithmic}[1]
    \Input
    \begin{itemize}
    	\item[--] image $\y$
    	\item[--] tree $\T(\V,\E)$
    	\item[--] codes $(\x^l)_{l \in \L}$
    	\item[--] supports $(\s^e)_{e \in \E}$
    \end{itemize}
    \Output kernels $(\h^e)_{e \in \E} \in \R^{|\E| \cdot N}$
    \State Initialize $(\h^e)_{e \in \E}$
    \While{not converged}
      \For{$e \in \E$ (depth-first way)}
      	\State $\h^e = \text{prox}^{\chi_{\Dspace^e}}_{t^e} \left(\h^e-\frac{1}{t^e} \nabla_{\h^e} \Phi[(\h^f)_{f \in \E}] \right)$
      \EndFor
    \EndWhile
  \end{algorithmic}
\end{algorithm}


\section{PALMTREE drawbacks}
As explained in \cite[p. 23]{chabiron_optimization_2016}, the main drawbacks to use this \gls{treemodel} for practical dictionary learning (i.e. denoising or image recognition) is that it requires the user to pre-define and fix certain central elements of the model, which is likely to lead to suboptimal results. Among them are the design of the tree (number of children per node, depth) and the choice of the supports.
% NOTE: dictionary learning is an application of machine learning specifically designed for image processing 
\subsubsection{Choice of the tree}
The authors of \cite{chabiron_optimization_2016} used a fixed tree structure, meaning that the tree has been created \emph{ad-hoc} on a per-experiment basis, trying to mimic the frequency pyramid tiling of a curvelet decomposition. The number of leaves was also specifically chosen to match the number of atoms that was generated on the target image.

But getting an actual adaptative dictionary update step implies that the design of the tree is also learned from the example images and incorporated into the formulation of the optimization problem. This research axis has not been explored in this work and will be part of future studies.

\subsubsection{Choice of the supports}

As described in \cref{sec_tree_model}, the supports $\s^e$ are part of the \gls{treemodel}. In \cite{chabiron_toward_2015}, the authors experimentally showed that using fixed supports for approximating many kinds of atoms (curvelets, wavelet packets) is possible. For the specific purpose of approximating existing atoms, the results turned out to be very promising. However, the proposed model was supposed to be able to approximate any kind of image, which is pratically not possible with fixed supports.

\subsubsection{Still a prototype}

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=\textwidth]{figures/tree-learn/target.pdf} 
	\caption{Handcrafted target image $\y$}
\end{subfigure}
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-learn/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-generic3x3_[fixed-supports]_tree.pdf}
	\caption{Learned tree defining $\D$}
\end{subfigure}
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-learn/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-generic3x3_[fixed-supports]_approx.pdf}
	\caption{Approximation $\D\x$}
\end{subfigure}
\caption{Example of PALMTREE solution. Supports are fixed on a simple binary tree binary. Codes $\x$ are handcrafted Diracs that exactly match the locations of atoms in $\y$.}\label{fig_exple_fixed_tree}
\end{figure}

Any actual image processing application would require to learn the dictionary on a large set of images (namely the $\Y$ matrix). However, \ac{PALMTREE} is still unable to handle more than one image.

Moreover, most experiments on \ac{PALMTREE} have been focused on the dictionary update step; real applications would need to alternate between the dictionary update step and the sparse coding step. In fact, without the sparse coding step, \ac{PALMTREE} is limited to images like the one in \cref{fig_exple_fixed_tree}, where the atoms are well separated and located. Therefore no “real world” images.

\FloatBarrier

\section{Objectives of present work}

So far, the supports of the convolutional tree dictionaries have been handcrafted. \Cref{fig_example_kernel} gives an example of typical support layout chosen for the experiments in \cite{chabiron_optimization_2016}. 

To fix ideas, \cref{fig_fixed_vs_expected} illustrates what could be expected to be found as the solution for an “adaptive” support of same size: the support (b) would shape itself into (c) to better match the target atom.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/manual-better-support/target.pdf}
	\caption{}\label{fig_fixed_vs_expected_y}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.5\textwidth]{figures/manual-better-support/support.pdf}
	\caption{Generic support}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/manual-better-support/support-better.pdf}
	\caption{Expected support}
\end{subfigure}
\caption{Generic versus expected supports. (a) is the target image made of one curvelet. (b) gives the locations of the support elements for one edge of the tree. (c) shows a handcrafted support that we expect to be giving better results as it follows the general direction of the curvelet.}\label{fig_fixed_vs_expected}
\end{figure}
\FloatBarrier

Finally, \cref{fig_xp_fixed_vs_expected} experimentally confirms the assumption made in \Cref{fig_fixed_vs_expected}. We switched $\s^4$ from a generic (\textit{a priori}) support to a hand-made support that follows the general direction of $\y$ (\cref{fig_fixed_vs_expected_y}). The RMSE\footnotemark[1] is decreased by -16\% (lower is better). The approximation in \cref{fig_xp_fixed_vs_expected_approx2} looks visually better than the one in \cref{fig_xp_fixed_vs_expected_approx1}.

Based on these observations, we propose in this work to make the supports “adaptative”, i.e. learning them during the optimization process.

\footnotetext[1]{Let $\y^1$ and $\y^2 \in \R^N$; the Root Mean Square Error is defined by  \begin{equation*}\text{RMSE}(\y^1,\y^2) = \frac{1}{N} \sqrt{\sum_{i=1}^N (y^1_i - y^2_i)^2}\end{equation*}}

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.085\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/tree_classic.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.39\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/xp_128x128_sc2_angl1_K3_S3_node4classic_approx.pdf}
	\caption{$\D\x$ with generic supports}\label{fig_xp_fixed_vs_expected_approx1}
\end{subfigure}
\begin{subfigure}[b]{0.085\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/tree_expected.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.39\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/xp_128x128_sc2_angl1_K3_S3_node4expected_approx.pdf}
	\caption{$\D\x$ with handcrafted $\s^4$}\label{fig_xp_fixed_vs_expected_approx2}
\end{subfigure}
\caption{Visual enhancement when adapting a support.} \label{fig_xp_fixed_vs_expected}
\end{figure}




\chapter{Experiments}

This chapter covers the protocols and results of the various experiments aimed to learning the supports during the \ac{PALMTREE} algorithm. We first review the experiments that helped us better understand the information given by the gradient. The second part focuses on our actual goal: learning the supports (instead of having fixed ones) on a single branch and finally learning supports on trees.

\section{Experiment setup}
Every experiment in the first part of this chapter is using a fixed single-branch tree of depth 4 associated with $\y$, $\h^e$, $\s^e$ and $\x^e$ of size $128 \times 128$. The figures only display the central $64 \times 64$ pixels as shown in \cref{fig_xp_explain}.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.9\textwidth]{figures/xp_explain/target.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.9\textwidth]{figures/xp_explain/dictionary.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}\centering
	\includegraphics[width=0.9\textwidth]{figures/xp_explain/code.pdf}
	\caption{}
\end{subfigure}
\caption{Experiment setup. The experiments are using one image $\y$, one dictionary $\D$ (formed by $\T$, $(\s^e)_{e \in \E}$, $(\h^e)_{e \in \E}$) and one $\x$. The parameters $\y$, $\x$, $\T$, $(\s^e)_{e \in \E}$ are given and $(\h^e)_{e \in \E}$ is learned (the only variable).}\label{fig_xp_explain}
\end{figure}
In the following experiments, the dictionary associated to this convolutional tree can be simply written as 
\begin{equation*}\D\x = \h^1 * \h^2 * \h^3 * \h^4 * x.\end{equation*}

\section{Using the gradient for choosing the element to add}

The Orthogonal Matching Pursuit (\ac{OMP}, detailed in \cref{alg_omp}) chooses at \cref{alg_omp_pick_correlation} which component $\x_i$ should be “turned on” at each iteration, finding the atom $\d_i$ that has the greatest correlation with the residual $\Res = \D\x - \y$, i.e.
\begin{equation*} i = \underset{j}{\argmax} ~ | \D^T \Res |_j \end{equation*}
which is litteraly the same as computing the gradient $\nabla \Phi(\x)$, with \begin{equation*}\Phi(\x) = \lVert \D\x - \y \rVert^2.\end{equation*}

\begin{algorithm}[!ht]
    \caption{Orthogonal Matching Pursuit (OMP) algorithm for sparse approximation}\label{alg_omp}
  \begin{algorithmic}[1]
    \Input signal $\y$ of dimension $N$, dictionary of dim. $N \times K$ with $K \gg N$
    \Output A $k$-sparse code $\x$ of dimension $K$
    \State \textbf{Initialization} $S=\{\}$
    \For{$i = 1,\dots,k$}
      \State $j =  \underset{j = 1,\dots,K}{\argmax}~| \nabla \Phi(\x)_j |$ \label{alg_omp_pick_correlation}\Comment{find atom with max. correlation with residual $\Res$}
      \State $S = S \cup \{j\}$
      \State $\x = \underset{\text{supp}(\x) \subset S}{\argmin} \Phi(\x)$
    \EndFor
  \end{algorithmic}
\end{algorithm}

Our idea is to use the same principle when adding elements to supports $(\s^e)_{e \in \E}$: add a new element to one of the supports between two minimizations of the \ac{PALMTREE} algorithm, using the greatest gradient component information.

\Cref{alg_omppalmtree} gives an adaptation of the \ac{OMP} algorithm for the \gls{treemodel}.

\begin{algorithm}[!ht]
    \caption{\ac{OMP} algorithm using \ac{PALMTREE}}\label{alg_omppalmtree}
  \begin{algorithmic}[1]
    \Input signal $\y$ of dimension $N$, dictionary of dim $N \times K$ with $K = |\E| \cdot N$ and $K \gg N$
    \Output A $k$-sparse code $\x$ of dimension $K$
    \State \textbf{Initialization} Initialize the supports $\s^e$ such that $k=1$
    \For{$i = 1,\dots,k$}
      \State $(e,p) = \underset{(e,p) \in \E \times \P}{\argmax}~ |\nabla \Phi_{\h^e}(\h)_p|$\Comment{Choose support $s^e$ and location $p$} \label{alg_omppalmtree_find}
      \State $\s^e_p = 1$
      \State $\h = \underset{\h^e \in \Dspace^e}{\argmin}~ \Phi(\h)$ \Comment{Solve \eqref{eq_ftl} using \ac{PALMTREE}} \label{alg_omppalmtree_ftl}
    \EndFor
  \end{algorithmic}
\end{algorithm}


However, three questions rose when designing this new “meta” algorithm:
\begin{enumerate}[label={\alph*)},noitemsep]
	\item At \cref{alg_omppalmtree_find}, is it actually correct to choose the element to be added using a strictly local information of the gradient?
	\item Also at \cref{alg_omppalmtree_find}, how would we get the “full” gradient while the current gradient is only computed on the points of the support?
	\item Finally, at (\cref{alg_omppalmtree_ftl}), is it really necessary to wait for the convergence of \eqref{eq_ftl}?
\end{enumerate}
  Also, 

We will first review how the “full” gradient is computed and then describe a way to check that the “full” gradient gives a good direction towards a critical point, comparing it with what we call “gain-per-added-point”. Finally, we will give some thoughts on whether we should wait for \eqref{eq_ftl} to converge or if some iterations suffice between two iterations of \cref{alg_omppalmtree}.


\subsection{Computing the “full” gradient} \label{sec_full_grad}
Up to now, the gradient was computed using a convolution based on translations, as mentioned in \cref{sec_palmtree}. It takes advantage of the sparsity of each kernel $\h^e$ as well as the fact that the proximal operator is projecting every kernel onto its support, meaning that only a few elements have to be computed.

We basically chose to use the fast Fourier transform ($\F$) for computing the convolution on the whole kernels:
\begin{align*}
	\nabla_{\h^e} \Phi((\h^f)_{f \in \E})&= \F^{-1}(\F(\H^{e})^* \circ \F(\Res))
\end{align*}
with ${}^*$ denoting the adjoint operator, $\circ$ the Hadamard product (point-wise product); $\H^{e}$ as well as $\Res$ are notations defined in \cref{sec_palmtree}.

\subsection{Validation of the “full” gradient}
We had some trouble when trying to compute the gradient on the full kernels $(\h^e)_{e \in \E}$ (not only on the supports element locations). To be sure that the computed “full” gradient was correct, we compared it to a finite-difference gradient:

\begin{equation*}\lim_{\epsilon \rightarrow 0} \frac{\Phi((\h^e)_{e \in \E}+\epsilon e_i) - \Phi ((\h^e)_{e \in \E}) }{\epsilon} ~=~ \nabla_i \Phi ((\h^e)_{e \in \E})\end{equation*}

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.30\textwidth}\centering
\includegraphics[width=\textwidth]{figures/verif_gradient/gradient.pdf}
\caption{“Full” gradient}
\end{subfigure}
\begin{subfigure}[b]{0.30\textwidth}\centering
\includegraphics[width=\textwidth]{figures/verif_gradient/finite-diff.pdf}
\caption{Finite-difference}
\end{subfigure}
\begin{subfigure}[b]{0.35\textwidth}\centering
\includegraphics[width=\textwidth]{figures/verif_gradient/finite-diff-vs-grad.pdf}
\caption{Error w.r.t. $\epsilon$}
\end{subfigure}
\caption{Checking that the “full” gradient is correct. We compare it to the finite-difference gradient and make the $\epsilon$ tend to 0.} \label{fig_verif_gradient}
\end{figure}

Figure \ref{fig_verif_gradient} compares the FFT-computed gradient to the finite-difference gradient (with $\epsilon=0.001$). The bottom left figure shows that the two gradients are very close in term of difference. The right figure shows that the finite-difference gradient converges to the “full” gradient, which confirms that our “full” gradient is correct.

\section{Gain-per-added-point to the support}\label{sec_gain_per_added_point}

This section details the method used for making sure that the “full” gradient is appropriate for adding points to supports at \cref{alg_omppalmtree_find} in \cref{alg_omppalmtree}.

\begin{algorithm}[!h]
    \caption{Gain-per-added-point $\g^f$ for the support $\s^f$}\label{alg_gain_per_added_point}
  \begin{algorithmic}[1]
    \Input One chosen edge $f \in \E$ and a tree $\T$, supports $\s$ and kernels $\h$
    \Output $\g^f$, the gain per added point for support $\s^f$
    \State $\k = \underset{\h}{\argmin}~ \Phi (\h)$ \quad s.t.~$\h^e \in \Dspace^e$ \quad $\forall e \in \E$ \Comment{common starting point}
    \For{each point $p$ of $\s^f$}
    	\State $\s^f_p = 1$ \Comment{add element to support}
    	\State Initialize $\h = \k$
    	\State $\g^f_p = \Phi(\k) - \underset{\h}{\min}~ \Phi (\h)$ \quad s.t.~$\h^e \in \Dspace^e$ \quad $\forall e \in \E$
    	\State $\s^f_p = 0$ \Comment{remove element to support}
    \EndFor
  \end{algorithmic}
\end{algorithm}

We designed \cref{alg_gain_per_added_point} for computing what we call the \emph{gain-per-added-point} matrix, denoted $\g^f$ ($f$ is the edge of the studied support). \Cref{alg_gain_per_added_point} tries every possible point that could be added to one given support $s^f$ and see how much the objective function decreases when adding the point and continuing the minimization.

\begin{figure}[!h]\centering
	\begin{subfigure}[b]{0.30\textwidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_target.pdf}
	\caption{Target $\y$}
	\end{subfigure}
	\begin{subfigure}[b]{0.34\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix.pdf}
	\caption{Gain-per-added-point for $\s^4$}
	\end{subfigure}
	\begin{subfigure}[b]{0.34\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4.pdf}
	\caption{Partial gradient w.r.t $\h^4$}
	\end{subfigure}
	\begin{subfigure}[b]{0.30\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_approx.pdf}
	\caption{Approximation $\D\x$} \label{fig_gain_n4_approx}
	\end{subfigure}
	\begin{subfigure}[b]{0.34\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix_bestvalues.pdf}
	\caption{Best values of above figure}
	\end{subfigure}
	\begin{subfigure}[b]{0.34\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n4/xp_128x128_sc2_angl1_K3_S3_node4_partgrad4_bestvalues.pdf}
	\caption{Best values of above figure}
	\end{subfigure}
\caption{Gain-per-added-point compared to “full” gradient. The “full” gradient is computed on the converged “common starting point” $\k$ (see \cref{alg_gain_per_added_point}). The gradient and gain-per-added-point maxima are almost at the same place. }\label{fig_gain_n4}
\end{figure}

\Cref{fig_gain_n4} compares (c) the gain-per-added-point of $\s^4$ to (d) the absolute value of the partial gradient w.r.t $\h^4$ after a convergence of \eqref{eq_ftl}. It is interesting to see that they share the same direction and shape. After selecting 20 of their best values in (e) and (f), we notice that they almost match, which reinforces our hypothesis that the gradient could give the right information for widening supports.

Note the null gradient values on the points of the support (small squares); with respect to those nine points, the necessary condition for optimality is met. However, the rest of the gradient is not null, meaning we are on a suboptimal solution.

\begin{figure}[!ht]\centering
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n1/xp_128x128_sc2_angl1_K3_S3_node1_objmatrix_bestvalues.pdf}
	\caption{$\s^1$ gain}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n1/xp_128x128_sc2_angl1_K3_S3_node1_partgrad1_bestvalues.pdf}
	\caption{$\h^1$ gradient}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n2/xp_128x128_sc2_angl1_K3_S3_node2_objmatrix_bestvalues.pdf}
	\caption{$\s^2$ gain}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp/n2/xp_128x128_sc2_angl1_K3_S3_node2_partgrad2_bestvalues.pdf}
	\caption{$\h^2$ gradient}
	\end{subfigure}
\caption{Two other edges (1 and 2) for experiment in \cref{fig_gain_n4}. The image $\y$ and its approximation $\D\x$ are identical to \cref{fig_gain_n4}.}\label{fig_gain_n1_n2}
\end{figure}

\Cref{fig_gain_n1_n2} does the same comparison for two other kernels of the same tree (it is the same experiment). Whatever the kernel, the best values of the gradient seem to follow the same shape as the gain-per-added-point matrix. 

\begin{figure}[!h]\centering
	\begin{subfigure}[b]{0.22\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_target.pdf}
	\caption{Target $\y$}
	\end{subfigure}
	\begin{subfigure}[b]{0.22\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_approx.pdf}
	\caption{Approx. $\D\x$}
	\end{subfigure}
	\begin{subfigure}[b]{0.26\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_partgrad4_bestvalues.pdf}
	\caption{$\s^4$ gain}
	\end{subfigure}
	\begin{subfigure}[b]{0.26\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/xp/tilted_n4/xp_128x128_sc2_angl4_K3_S3_node4_objmatrix_bestvalues.pdf}
	\caption{$\h^4$ gradient}
	\end{subfigure}
\caption{Trying a different $\y$.}\label{fig_gain_tilted_n4}
\end{figure}

Finally, \cref{fig_gain_tilted_n4} presents the same experiment but with a different atom (this time, a curvelet with a different angle). And as for the two previous figures, the gradient matches the gain-per-added-point.

\subsection{Visual gain when adding an element}

\begin{figure}[!h]\centering
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_target.pdf}
	\caption{Target image}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_approx.pdf}
\caption{Before adding}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_approx.pdf}
\caption{After adding}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_objmatrix.pdf}
\caption{Gain-per-added-point} \label{fig_gain_matrix}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_partgrad4.pdf}
\caption{Gradient before} \label{fig_grad_before}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_partgrad4.pdf}
\caption{Gradient after} \label{fig_grad_after}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_objmatrix_bestvalues.pdf}
\caption{Best values of \ref{fig_gain_matrix}}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4before_partgrad4_bestvalues.pdf}
\caption{Best values of \ref{fig_grad_before}}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}\centering
\includegraphics[width=\linewidth]{figures/before_after/xp_128x128_sc2_angl1_K3_S3_node4after_partgrad4_bestvalues.pdf}
\caption{Best values of \ref{fig_grad_after}}
\end{subfigure}
\caption{Effect of adding one point to the support $\s^4$. The “before” tree is a converged solution of \ac{PALMTREE}. The “after” tree has been added a point where the gain-per-added-point was maximal.}\label{fig_before_after_adding}
\end{figure}

The experiment shown in \cref{fig_before_after_adding} is based on the results of \cref{fig_gain_n4_approx}, where figures (a,b,d,e,g,h) are the same as the previous experiment. The figures (c,j,i) show what  happens if we add the minimum point of gain-per-added-point to $\s^4$ and continue the minimization. We note that the approximation (c) looks visually closer to the target image; the RMSE is decreased by -16\% (lower is better).

We also observe that when adding a point to the support (materialized by the \nth{10} small square), the highest gradient values “shift” to the south-west. Adding a point to the support has an effect on every surrounding points.

\FloatBarrier
\subsection{Adding elements before the convergence of \ac{PALMTREE}} \label{sec_add_before_converged}

At \cref{alg_omppalmtree_ftl} of \cref{alg_omppalmtree}, we do a full minimization, i.e. it stops as soon as \ac{PALMTREE} has converged. Because the minimization takes a lot of time, it was tempting to try adding elements every few iterations of \ac{PALMTREE} instead of waiting until it converges.

The \cref{fig_gain_n4} has shown that the “full” gradient could be satisfactorily used for adding elements to the support $s$, provided that \cref{alg_palmtree} at \cref{alg_omppalmtree_ftl} has converged. Would the gradient still give a valid information on a not-yet converged solution $\k$ of \ac{PALMTREE}, for example after 5 iterations?

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.49\linewidth}\centering
\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_objmatrix_bestvalues.pdf}
\caption{Gain-per-added-point matrix}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}\centering
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter1_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 1}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter8_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 8}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter20_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 20}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}\centering
	\includegraphics[width=\linewidth]{figures/xp_grad_iterations/xp_128x128_sc2_angl1_K3_S3_node4_iter200_partgrad4_bestvalues.pdf}
	\caption{Grad. iter. 200}
	\end{subfigure}
\end{subfigure}
\caption{Is the gradient giving the same information depending on the iteration of \ac{PALMTREE}? This is the same experiment as \cref{fig_gain_n4}. Only the best values are displayed.}\label{fig_iter_gain_vs_grad}
\end{figure}

\Cref{fig_iter_gain_vs_grad} compares the best values of gain-per-added-point to gradient snapshots taken during the minimization. This is reassuring: whatever the iteration, the gradient best values stay close to the gain-per-added-point best values. We may conclude that adding elements every few iterations of \ac{PALMTREE} works.

\FloatBarrier
\section{Learning the supports for a single branch}

After having shown that the gradient gives a good indication on which support elements should be added, we tried learning the support $\s$ using the \cref{alg_omppalmtree}. According to the conclusions of \cref{sec_add_before_converged} that indicates that the convergence of \ac{PALMTREE} is not mandatory before adding an element, we simply set a smaller maximum of iterations for \ac{PALMTREE}; we denote this number of iterations $T$.

\begin{figure}[!ht]\centering
\begin{subfigure}[b]{0.085\textwidth}\centering
	\includegraphics[width=\textwidth]{figures/exple-better-support/tree_classic.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.39\textwidth}\centering
\includegraphics[width=\textwidth]{figures/exple-better-support/xp_128x128_sc2_angl1_K3_S3_node4classic_approx.pdf}
\caption{Approx. $\D\x$ with generic supports}\label{fig_xp_fixed_vs_expected_approx1}
\end{subfigure}
\begin{subfigure}[b]{0.085\textwidth}\centering
\includegraphics[width=\textwidth]{figures/variable_support/support.pdf}
\caption{}\label{fig_variable_support_c}
\end{subfigure}
\begin{subfigure}[b]{0.39\textwidth}\centering
\includegraphics[width=\textwidth]{figures/variable_support/xp_128x128_sc2_angl1_K3_S3_node4_variable_approx.pdf}
\caption{Approx. $\D\x$ with learned supports} 
\end{subfigure}
\caption{Learning the supports for a single branch. Starting from Diracs, the best element is added to one of the supports every 5 iterations, up to 36 elements.}\label{fig_variable_support}
\end{figure}

In \cref{fig_variable_support_c}, the initial supports are simply centered Diracs; every $T=5$ iterations of \ac{PALMTREE}, one element is added where the gradient is the greatest, up to 36 elements. The experiment converges after about 200 iterations of \ac{PALMTREE}.

The resulting approximation (d) is encouraging with a relative RMSE decrease of -76\% compared to the approximation (b) using the generic supports in (a).

%\begin{subfigure}[b]{1\textwidth}\centering
%\includegraphics[width=\textwidth]{figures/variable_support/xp_128x128_sc2_angl1_K3_S3_node4_variable_target.pdf}
%\caption{Image $\y$}
%\end{subfigure}

\FloatBarrier
\section{Learning supports for a tree}

\subsection{Experiment setup}

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=\textwidth]{figures/tree-learn/target.pdf} 
	\caption{Target image $\y$}
\end{subfigure}
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=\textwidth]{figures/tree-learn/tree.pdf}
	\caption{Tree defining $\D$}
\end{subfigure}
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=\textwidth]{figures/tree-learn/codes.pdf} 
	\caption{Some codes $\x^l$ (4 among 16)}
\end{subfigure}
\caption{Setup of the support learning experiments.}\label{fig_learntree_setup}
\end{figure}

For testing our support learning algorithm \cref{alg_omppalmtree}, we use the convolutional tree defined in \cref{fig_learntree_setup}. The target $\y$ made of curvelets at different scales and angles. From left to right and top to bottom, the 16 curvelets of $\y$ (numbered 1 through 16) have the corresponding codes $\x^i$ with $i=1,\dots16$. The codes are located at the center of their corresponding curvelet.


\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.8\textwidth]{figures/tree-scattered-supports/xp_learnsupp256_curvelet_decomp3+tree-binary_dpth4+supp-diracs+usegrad0_every5_add5_totinit0_totadd279_a0_b1_approx.pdf}
\caption{Approximation $\D\x$}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.8\textwidth]{figures/tree-scattered-supports/xp_learnsupp256_curvelet_decomp3+tree-binary_dpth4+supp-diracs+usegrad0_every5_add5_totinit0_totadd279_a0_b1_tree.pdf}
	\caption{Learned support tree} \label{fig_test_omppalmtree_tree}
\end{subfigure}
\caption{Results after using \cref{alg_omppalmtree}. The support elements in (b) are extremely scattered, thus making them indistinguishable.}\label{fig_test_omppalmtree}
\end{figure}

A first result using \cref{alg_omppalmtree} as-is is displayed in  \Cref{fig_test_omppalmtree}. The support elements are so distant from each other that we cannot distinguish them. A zoom on one of the supports is made in the next section.



\subsection{Problem of scattered elements using \cref{alg_omppalmtree}}
 
An unforeseen issue rose when trying the \cref{alg_omppalmtree} on a tree instead of a single branch: the gradient indicates elements that are far from each other. The selection of the best support element to be added can be unexpectedly far from previously added elements, hence producing scattered kernels. Having “gathered” supports is essential for the convolutional tree model. \todo[inline]{HELP: I can’t find explanations on why it is essential to have “gathered” kernels, contrary to “scattered” kernels} 

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.65\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-scattered-supports/supports_with_zoom.pdf}
\caption{Supports with magnified $\s^1$} \label{fig_scattered_support_tree}
\end{subfigure}
\begin{subfigure}[b]{0.34\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-scattered-supports/_partgrad1.pdf}
\caption{Gradient for $\h^1$} \label{fig_scattered_support_grad}
\end{subfigure}
\caption{Scattered supports when using plain \cref{alg_omppalmtree}. The gradient in (b) has its maximum values in distant locations, therefore producing scattered supports.} \label{fig_scattered_support}
\end{figure}

\Cref{fig_scattered_support_tree} illustrates this problem; every support has its elements scattered instead of being gathered in one place. The corresponding gradient, shown in \cref{fig_scattered_support_grad}, has multiple “local” maxima that are far from the origin. Ideally, we would like all elements to be centered around the origin (in the figures, the origin is at the center of the image).

\begin{algorithm}[!h]
    \caption{\ac{OMP}-\ac{PALMTREE} (based on \cref{alg_omppalmtree})}\label{alg_omppalmtree2}
  \begin{algorithmic}[1]
    \Input signal $\y$ of dimension $N$, dictionary of dim $N \times K$ with $K = |\E| \cdot N$ and $K \gg N$
    \Output A $k$-sparse code $\x$ of dimension $K$
    \State \textbf{Initialization} Initialize the supports $\s^e$ such that $k=1$
    \For{$i = 1,\dots,k$}
      \State $(e,p) = \underset{(e,p) \in \E \times \P}{\argmax}~ \left[ |\nabla \Phi_{\h^e}(\h)_p| \underbrace{ - \alpha~f(p)}_{\text{Concentrate the support}} \right]$ \label{alg_omppalmtree2_find}
      \State $\s^e_p = 1$
      \State $\h = \underset{\h^e \in \Dspace^e}{\argmin}~ \Phi(\h)$ \label{alg_omppalmtree2_ftl}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-scattered-supports/grad_node1.pdf}
\caption{Gradient } \label{}
\end{subfigure}
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-scattered-supports/dist_to_orig.pdf}
\caption{Function $f(p)$} \label{}
\end{subfigure}
\begin{subfigure}[b]{0.325\textwidth}\centering
\includegraphics[width=1\textwidth]{figures/tree-scattered-supports/regularized.pdf}
\caption{gradient - $f(p)$} \label{}
\end{subfigure}
\caption{Example of linear regularization using distance to origin (center of the figure).} \label{fig_grad_minus_dist}
\end{figure}

To address this issue, we added in \cref{alg_omppalmtree2} a regularization (or prior) term $f(p) \subset \R$ that prevents $p$ from being too far from the origin (center\footnote{The actual implementation relies on kernels centered on the origin $(1,1)$; for easier understanding, we translate the kernels so that the origin is centered.} of the support).
\begin{equation}
f(p) = \frac{\text{dist}(o-p)}{\underset{p' \in 1,\dots,N}{\max}\text{dist}(o-p')} \quad \in [0,1] \label{eq_regularized1}
\end{equation}
with $o$ the origin, which is the center of the support. Note that $f$ simply normalizes \text{dist()} in $[0,1]$. \Cref{fig_grad_minus_dist} illustrates the effect on the regularization $f(p)$ on the objective function (at \cref{alg_omppalmtree2_find} of \cref{alg_omppalmtree2}).


  $\text{dist}(p)$ is the circular euclidian norm defined on $p=1,\dots,N$ by\footnotemark[1]
\begin{equation*} \text{dist}(p) = \min\left[p \bmod N, (N-p) \bmod N\right]\end{equation*}
with $\bmod$ the modulo operator.

\footnotetext[1]{Note that the actual implementation uses non-vectorized images (matrices). This implies that this definition must be extended to two dimensional $\bm{p} = \begin{bmatrix}
	p_1 \\ p_2\end{bmatrix}$. It is easily done with 
	$\text{dist}(\bm{p}) = \left\| \begin{bmatrix} \text{dist}(p_1) \\ \text{dist}(p_2)\end{bmatrix} \right\|_2$.}



\subsection{Tuning the regularization parameter $\alpha$}
In the previous section, we proposed the regularization function $f$, which means that we must deal with an additional parameter $\alpha$.

\subsubsection{Ad-hoc tuning of $\alpha$}
\begin{figure}[!h] \centering
\includegraphics[width=1\textwidth]{figures/tree-scattered-supports/gradient_used.pdf}
	\caption{Blind tuning of $\alpha$}
\end{figure}

An ad-hoc way of finding the right $\alpha$ is to try every possible value. We launched the algorithm for every $\alpha$ ranging from 0 to 70. \Cref{fig_unbalanced_supports} compares the RMSE and compute time with respect to $\alpha$. The optimal $\alpha$ seems to be around 10.
\Cref{fig_test_omppalmtree2} gives the result for $\alpha=10$ and $\alpha=30$. Compared to \cref{fig_test_omppalmtree}, the regularization with $\alpha=10$ improves the RMSE by -47\% (-29\% for $\alpha=30$). It confirms our intuition that “gathered” supports lead to better approximations.


% (0.0079267-0)/0.0079267

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.8\textwidth]{figures/tree-unbalanced-supp/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-diracs_[usegrad1_every5_add5_totinit0_totadd279_alpha10]_approx.pdf}
\caption{Approximation $\D\x$ ($\alpha=10$)}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.8\textwidth]{figures/tree-unbalanced-supp/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-diracs_[usegrad1_every5_add5_totinit0_totadd279_alpha10]_tree.pdf}
\caption{Learned support tree ($\alpha=10$)}
\end{subfigure}
\end{figure}
\begin{figure}[!h] \ContinuedFloat
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.8\textwidth]{figures/tree-unbalanced-supp/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-diracs_[usegrad1_every5_add5_totinit0_totadd279_alpha30]_approx.pdf}
\caption{Approximation $\D\x$ ($\alpha=30$)}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.8\textwidth]{figures/tree-unbalanced-supp/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-diracs_[usegrad1_every5_add5_totinit0_totadd279_alpha30]_tree.pdf}
\caption{Learned support tree ($\alpha=30$)}
\end{subfigure}
\caption{Learned dictionary using \cref{alg_omppalmtree2} with $\alpha=10$ and $\alpha=30$.} \label{fig_test_omppalmtree2}
\end{figure}

%\begin{algorithm}[!h]
%    \caption{Bruteforce method for estimating $\alpha$}\label{alg_alpha_bruteforce_method}
%  \begin{algorithmic}[0]
%  	\State Set $\alpha$ to a large value, set $r < 1$
%  	\Repeat
%	  	\State Run OMP-PALMTREE using $\alpha$
%	  	\State $\alpha \leftarrow r\alpha$
%  	\Until{stopping criteria is met}
%\end{algorithmic}
%\end{algorithm}

This brute-force method could be a possible way of tuning the $\alpha$. However, it requires many runs of OMP-PALMTREE. The average time of one run being about 200 seconds, using the algorithm is not conceivable.

\subsubsection{Estimating $\alpha$ from the data}

The regularization parameter $\alpha$ should be estimated from the target image, meaning that finding the right $\alpha$ should be a part of the algorithm. Many methods are available in the literature; one of the simplest is the Blind Method proposed in \cite{almeida_parameter_2013}. We can also mention the state-of-the-art SURE method (Stein’s unbiased risk estimate), proposed in \cite{eldar_generalized_2009} as well as the bayesian regularization method developed in \cite{archer_bayesian/regularization_1995}.

These algorithms have not been investigated during this internship and will be conducted in a PhD thesis.

\subsection{Regularization problem: unbalanced supports} \label{sec_pbm_unbalanced_supports}
The regularization allowed us to prevent scattered supports; another problem probably caused by the gradient is the “unbalanced” supports: some supports contain most elements, while others have only one or two elements. 

\begin{figure}[!h] \centering
\begin{subfigure}[b]{1\textwidth}\centering
\includegraphics[width=0.6\textwidth]{figures/tree-unbalanced-supp/tree-unbalanced.pdf}
\end{subfigure}
\caption{Problem of unbalanced support choosing ($\alpha=10$).} \label{fig_unbalanced_supports}
\end{figure}

% The reason why we need “balanced” supports is linked to another research axis we would like to explore: learning the tree. In fact, if a support becomes a single Dirac, we can remove the edge it is associated with. This means that learning the tree 

The reason why we need “balanced” supports is that an overly-complex support (like the middle one in \cref{fig_unbalanced_supports}) makes the convolution slow. The more uniformly distributed the support elements are, the faster the convolution.

% In the couple $(e,p)$ chosen by the gradient, the choice of the support edge $e$ does not seem to effective, although the choice of $p$ is valid (as discussed in \cref{sec_gain_per_added_point}).

\Cref{fig_unbalanced_supports} illustrates this issue: with $\alpha=10$, most elements are added on the support $\s^1$, while $\s^{27}$ has only 3 elements.

We found out that adjusting the regularization parameter $\alpha$ is not enough if we want “balanced” supports. We could simply make $\alpha$ dependent of $e$, which could lead to the following “find support element to add” step
\begin{equation}
(e,p) = \underset{(e,p) \in \E \times \P}{\argmax}~ |\nabla \Phi_{\h^e}(\h)_p| - \alpha_e f(p)
\end{equation}
with $\bm{\alpha} = \begin{bmatrix}\alpha_1 & \dots & \alpha_{|\L|}\end{bmatrix}$.

Estimating this vector $\bm{\alpha}$ requires even more complex algorithms. We will let this possibility to future studies. 

\subsubsection{Empirical workaround using one single $\alpha$}
Instead of estimating the vector $\bm{\alpha}$, we empirically developed a function $f$ for balancing the number of elements per support using one single $\alpha$. This method requires $\alpha$ to be estimated, though.

The regularization function is
\begin{equation}
f’(\h,e,p) = \frac{\text{dist}(o-p)}{\underset{p' \in \P}{\max}\text{dist}(o-p')} \cdot G(\h,e) \quad \in [0,G(\h,e)]\label{eq_regularized1}
\end{equation}
where $G(\h,e)$ is the maximum value of the gradient for the kernel $h^e$; we define it as 
\begin{equation*} 
G(\h,e)=\underset{p' \in \P}{\max} |\nabla \Phi_{\h^e}(\h)_{p'}|.
\end{equation*}

This regularization function is simply adjusted with respect to the amplitude of each partial gradient. This result is totally empirical and has not been proved (it may even lead to suboptimal supports).

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/gradient-vs-sequential/xp_learnsupp256_curvelet_decomp3_tree-binary_dpth4_supp-diracs_usegrad1_every5_add5_totinit0_totadd279_alpha30_tree.pdf}
	\caption{Supports chosen using gradient} \label{fig_cmp_grad_tree}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/gradient-vs-sequential/xp_learnsupp256_curvelet_decomp3_tree-binary_dpth4_supp-diracs_usegrad0_every5_add5_totinit0_totadd279_alpha30_tree.pdf} 
	\caption{Supports chosen sequentially} \label{fig_cmp_seq_tree}
\end{subfigure}
\end{figure}
\begin{figure}[!h] \ContinuedFloat
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/gradient-vs-sequential/xp_learnsupp256_curvelet_decomp3_tree-binary_dpth4_supp-diracs_usegrad1_every5_add5_totinit0_totadd279_alpha30_approx.pdf}
	\caption{Approximation $\D\x$ of above tree} \label{fig_cmp_grad_approx}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/gradient-vs-sequential/xp_learnsupp256_curvelet_decomp3_tree-binary_dpth4_supp-diracs_usegrad0_every5_add5_totinit0_totadd279_alpha30_approx.pdf} 
	\caption{Approximation $\D\x$ of above tree} \label{fig_cmp_seq_approx}
\end{subfigure}
\caption{Comparison of sequential versus using gradient when choosing a support for adding an element.}\label{fig_cmp_seq_vs_grad}
\end{figure}

We compared in \cref{fig_cmp_seq_vs_grad} the results using $f’$ against a modified version of OMP-PALMTREE where the elements are added sequentially on every support (only $p$ is determined with the gradient).
\begin{enumerate}[label=(\alph*)]
	\item $e$ and $p$ are both chosen using the gradient (\cref{fig_cmp_grad_tree});
	\item $p$ is chosen using the gradient, $e$ is sequentially selected: we add an element to $\s^1$, then to $\s^2$\dots so that every support has the exact same number of elements (\cref{fig_cmp_seq_tree}).
\end{enumerate}

Sequentially choosing the support for adding a new element in \cref{fig_cmp_grad_tree} obviously leads to uniformly distributed supports. And as we wanted, using $f’$ also gives more of less balanced supports. Curiously, choosing the supports sequentially gives better results in terms of RMSE.

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/tree-learn/gradient-vs-sequential/gradient_used.pdf}
	\caption{Using $f’$ regularization} \label{fig_cmp_rmse_vs_time_grad}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/tree-learn/gradient-vs-sequential/gradient_not_used.pdf} 
	\caption{Using the sequential variant}\label{fig_cmp_rmse_vs_time_seq}
\end{subfigure}	
\caption{RMSE versus compute time of OMP-PALMTREE for many $\alpha$} \label{fig_cmp_rmse_vs_time}
\end{figure}

\Cref{fig_cmp_rmse_vs_time} compares the RMSE to the time spent for many $\alpha$ from 0 to 70, as in \cref{sec_pbm_unbalanced_supports}. 

For both \cref{fig_cmp_rmse_vs_time_grad} and \cref{fig_cmp_rmse_vs_time_seq}, the best $\alpha$ is located in the range from 20 to 30. It is interesting to note that choosing the supports sequentially significantly reduces the compute time as well as the RMSE; this result was unexpected.

\begin{figure}[!h] \centering
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-generic3x3_[fixed-supports]_tree.pdf}
	\caption{Tree with (fixed) generic supports}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/gradient-vs-sequential/xp_learnsupp256_curvelet_decomp3_tree-binary_dpth4_supp-diracs_usegrad1_every5_add5_totinit0_totadd279_alpha30_tree.pdf} 
	\caption{Tree with learned supports, $\alpha=30$(87 secondes)}
\end{subfigure}
\end{figure}
\begin{figure} \ContinuedFloat
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/xp_learnsupp256_curvelet_decomp3[tree-binary_dpth4]_supp-generic3x3_[fixed-supports]_approx.pdf}
	\caption{Approximation $\D\x$ (generic supports)}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=0.9\textwidth]{figures/tree-learn/gradient-vs-sequential/xp_learnsupp256_curvelet_decomp3_tree-binary_dpth4_supp-diracs_usegrad1_every5_add5_totinit0_totadd279_alpha30_approx.pdf} 
	\caption{Approximation $\D\x$ (learned supports)}
\end{subfigure}
\caption{Comparison of generic versus learned supports. The RMSE decreases by -58\%.}\label{fig_learnsupp_vs_generic}
\end{figure}

Finally, \cref{fig_learnsupp_vs_generic} shows the difference between  generic supports (left) and a learned supports using $f’$ with $\alpha=30$ (right). As expected, learning the supports strongly decreases the RMSE (-58\%); the compute times are equivalent (87 seconds with no retries and same stopping criteria).


\FloatBarrier
\section{Conclusions and perspectives}

 
During this study, we found out that the gradient can be used for learning the support in an \ac{OMP} manner, alternating between adding element supports and minimizing with PALMTREE. We experimentally showed that we could add multiple elements at each OMP iteration, and that it wasn’t necessry to wait until PALMTREE convergence before going to the next OMP iteration. 
Using these  also noticed that learning supports on a branch gives excellent results, although being less conclusive on a tree. After some adaptations, the algorithm OMP-PALMTREE gave promising results for curvelets – with the downside of having a new parameter to tune.

These comforting results on simple curvelet targets should be deepened using other kinds of atoms. In \cite{chabiron_optimization_2016}, wavelet packets have been used as target atoms. Testing our algorithms on different atoms would give us more feedback on how to tune the support learning.

Before beginning this study, we had in mind an algorithm for support learning that would add elements – as we actually did – but also remove useless elements. This part has not been studied, even though some of the implementation has been done (for removing elements).

Beyond this internship, the idea was to learn most parameters for the \gls{treemodel}; including the supports and the tree structure. In our experiments, we have let aside the tree structure, using a simple binary tree that would fit our needs. But a tiling-like tree, as shown in \cite{chabiron_optimization_2016}, would perform much better. 

Also, this work only focuses on the dictionary update step – this is why we use simplified codes and targets with well separated atoms. For this model to be useful in real applications, we should integrate it to a full dictionary learning algorithm that includes a sparse coding step. This has been investigated in \cite{chabiron_optimization_2016}.

Finally, the whole \gls{treemodel} has only been designed for learning on a single image $\y$; a future goal would be to modify the algorithm to use multiple images when learning, as well as make it scalable (like the Online Dictionary Learning algorithm).


\clearpage
\addcontentsline{toc}{chapter}{Appendix}
\appendix

\chapter{}

\section{Detail of the \ac{KSVD} algorithm} \label{sec_ksvd_detail}
\begin{algorithm}
    \caption{\ac{KSVD} (K-Singular Value Decomposition) algorithm for \eqref{eq_dl_ksvd}} \label{alg_ksvd}
  \begin{algorithmic}[1]
    \Input signal samples $\y_i$ of dimension $N$ forming the $S$ columns of $\Y$
    \Output dictionary $\D$ of dim. $K \times N$ with $K \gg N$
    \State \textbf{Initialization} Initialize $\D$ such that every column is $l^2$ normalized
    \While{not converged}
	\State For each image $\y_i$, solve \Comment{\textbf{Sparse Coding step}} \label{alg_ksvd_sparse_coding}
		\begin{align*}
			\quad \min_{\x_i}~ & \lVert \y_i-\D\x_i\rVert^2 \quad \text{s.t.}~ \lVert \x_i \rVert_0 \le \gamma
		\end{align*}
	\State $\D^{old} = \D$
	\State For each atom $\d_k^{old}$ of $\D^{old}$, \Comment{\textbf{Dictionary Update step}}\label{alg_ksvd_dict_update}
	\begin{enumerate}[leftmargin=15mm,label=(\alph*)]
		\item find the images $\y_i$ that use the atom $\d_k^{old}$, meaning $\x_i^k \ne 0$ \label{item_atom_used}
		\item compute the error matrix associated to this atom,
		\begin{align*}
			\bm{E}_k = \Y-\sum_{i\ne k} \x_i^T\d_i^{old}
		\end{align*}
		\item keep only columns of $\bm{E}_k$ found in \cref{item_atom_used}
		\item Apply SVD decomposition $\bm{E}_k - \bm{U} \Delta V^T$. The first column of $\bm{U}$ becomes the new $\d_k$. $\x_k$ is updated using the first column of $\bm{V}$ multiplied by $\Delta(1,1)$ (highest eigenvalue)
	\end{enumerate}
    \EndWhile
  \end{algorithmic}
\end{algorithm}



\FloatBarrier
\section{Link between \gls{treemodel} and dictionary matrix}\label{sec_matrix_vs_tree} 

It is useful to introduce the circulant matrix when trying to translate the convolutional product into a matrix-vector product. The circulant matrix of a vector $\h$ of dimension $N$ is defined as the $N \times N$ matrix $C(\h)$ such that
\begin{equation*}C(\h) = 
	\begin{bmatrix}
		h_1     & h_{N}   & h_{N-1} & \dots & h_{2} \\
		h_2     & h_{1}   & h_{N}   & \dots & h_{3} \\
		h_3     & h_{2}   & h_{1}   & \dots & h_{4} \\
		\vdots  &         &         &       & \vdots \\
		h_{N-1} & h_{N-2} & h_{N-3} & \dots & h_{N} \\
		h_N     & h_{N-1} & h_{N-2} & \dots & h_{1} \\
	\end{bmatrix}
.\end{equation*}

\begin{figure}[!ht] \centering
\includegraphics[width=\textwidth]{figures/block-circulant_matrix.pdf}
\caption{Rewriting the convolution into a metrix-vector product.} \label{fig_block_circular}
\end{figure}

We can then rewrite the convolutions used in
\begin{equation*}\D\x = \sum_{l \in \L} \x^l * \h^{*l}\end{equation*}
using circular matrices. We denote $C^i = C(\h^i)$ the circular matrix for edge $i$. \Cref{fig_tree_as_matrix} compares the convolutional tree (a) to the product $\D\x$ ($\D$ being the dictionary $\D$ written as a matrix). For the convolutional tree given in (a), $\D$ can be summarized as
\begin{align}
\D = \begin{bmatrix}C^5 & C^6\end{bmatrix} \begin{bmatrix}C^1 & C^2 & 0 & 0 \\0 & 0 & C^3 & C^4\end{bmatrix} \label{eq_matrix_developed}
\end{align}
with $C^i$ the block-circular matrix for $\h^i$; it represents the final convolution $\h^{*l}*\x^l$ when using matrices.

\begin{figure}[!ht] \centering
\begin{subfigure}[b]{0.30\textwidth}\centering
\includegraphics[width=0.7\textwidth]{figures/pov-tree.pdf}
\caption{Convolutional tree}
\end{subfigure}
\begin{subfigure}[b]{0.69\textwidth}\centering
\includegraphics[width=\textwidth]{figures/pov-matrix.pdf}
\caption{Convolutional tree dictionary using a circular matrix}
\end{subfigure}
\caption{From $\D$ as a convolutional tree to $\D$ as a matrix}\label{fig_tree_as_matrix}
\end{figure}

One atom $d_i$ with $i = 1,\dots,K$ ($K=N \cdot |\L|$) of the dictionary matrix $\D$ corresponds to one column of the block-circulant matrix $C(h^{*l})$; $\D$ can thus be rewritten as
\begin{align}\D = \begin{bmatrix} C(h^{*1}) & \dots & C(h^{*{|\L|}}) \end{bmatrix}.\label{eq_matrix_factorized} \end{align}

To sum up, the matrix associated to the convolutional tree dictionary $\D$ can be understood as a factorization of sparse matrices $C^i$. Under the “developed” form \eqref{eq_matrix_developed}, the matrix $\D$ is of dimension $N \times K$.




\FloatBarrier
\section{Why is sparse coding used for denoising?}

The figure \ref{sparse_reduce_noise} shows a noisy signal $\y$ living in a high-dimensional subspace of dimension $N$ and defined by
\begin{equation*}\y=\hat{\y} + \bm{b}\end{equation*}
with $\bm{b}$ following a centered Gaussian distribution $\mathcal{N}(0,\sigma^2I)$ and $\hat{\y}$ the noiseless signal. We see that the distance $||\bm{b}||$ is always smaller than the projected distance.

This is because the variance in the $N$ dimensional space can be written as
\begin{align*}
\sigma^2(\bm{b}) =& \mathbb{E}\left[\lVert \bm{b}-\mathbb{E}(\bm{b}) \rVert^2 \right]\\
=& \mathbb{E}\left(\lVert \bm{b} \rVert^2 \right)\\
=& \mathbb{E}\left(\sum^N_{i=1} b_i^2 \right)\\
\shortintertext{And as the variance is also $\sigma^2(\bm{b})= \frac{1}{N}\left(\sum_{i=1}^N b_i^2 \right) - \mathbb{E}(\bm{b})^2$, it becomes}
=& \mathbb{E} \left(N\sigma^2(\bm{b}) + N\mathbb{E}(\bm{b})^2\right)\\
=& N \mathbb{E} \left(\sigma^2(\bm{b})\right)\\
=& ... \\
=& \frac{1}{N}\lVert \bm{b} \rVert^2
\end{align*}\todo{Ask for help}
which gives 
\begin{equation*} \lVert \bm{b} \rVert = \sigma\sqrt{N} \end{equation*}
When projected onto the $k$ dimensional space, the noise deviation becomes
\begin{equation*}\lVert \bm{b} \rVert = \sigma\sqrt{k} \end{equation*}
which is much better than the previous distance, provided that $k \ll N$. 

\begin{figure}[!ht]\centering
\includegraphics[width=0.4\textwidth]{figures/sparse-reduce-noise.pdf}
\caption{When projected onto a lower dimensional space, the standard deviation of the additive Gaussian noise $\bm{b} \sim \mathcal{N}(0,\sigma^2I)$ will be greatly reduced if $k \ll N$. The sparser the representation the better the denoising. \label{sparse_reduce_noise}}
\end{figure}

\FloatBarrier

\section{Prooving $\F(\widetilde{A}) = \F(A)^*$} \label{sec_proof_fourier_flip_adjoint}
When dealing with periodic images (images are periodically repeated such that every point $p \in \mathcal{Z}$ is defined), computing the Fourier transform of a “flipped” image $\F(\widetilde{A})$, with $\widetilde{A}_p = A_{-p}$, can be simplified using the property

\begin{equation*}\F(\widetilde{A}) = \F(A)^*\end{equation*}

with $\F$ the \ac{DFT}, $^*$ the adjoint operator. Here is the proof:
\begin{align*}
\F({\widetilde{A}})_{m,n} =& \F(A)_{-m,-n} \\
=& \sum_{k=1}^M \sum_{l=1}^N A_{k,l} e^{-2i\pi (-k\frac{m}{M}-l \frac{n}{N})}\\
=& \sum_{k=1}^M \sum_{l=1}^N A_{k,l} \left( e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})} \right)^*\\
=& \left( \sum_{k=1}^M \sum_{l=1}^N A_{k,l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})} \right)^*\\
=& \left( \F(A)_{m,n} \right)^*
\end{align*}

%\section{Performance of learning the transform and the transform itself}
%\todo{Is the transform parallelizable? Is learning the transform parallelizable?}


\printglossary
{\let\clearpage\relax \printacronyms}
\addcontentsline{toc}{chapter}{References}\printbibliography[title=References]


\end{document}



