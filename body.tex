%!TEX root = main.tex
%\pagestyle{corps}


%\chapter*{Thanks \markboth{Thanks}{}} 
%\begin{flushright}MaÃ«l\end{flushright}


Many image processing techniques are based on... representing the signal ...

We aim... 

Now that it has been proven that the objective function, although non-convex, actually allows to find local minimums that are close enough to the actual minimum.


The idea behind dictionaries
Say $y$ is a signal we want to ...
We want to find a transform/dictionary D such that, when applied to a code (think of the "transformed" signal) will give an approximated signal $\hat{y}$:

$$D\alpha=\hat{y} \approx y$$

In this work, we will generally stay on the non-noisy case, leaving noisiness to further work.

Algorithm PALMTREE: a Gauss-Seidel algorithm (in the sense that we 

In dictionary learning, we call the columns of $D$ the \emph{atoms}.


This experiment aims to simulate the choice of an atom by the OMP algorithm. As a short reminder, here is the step we are studying:
\begin{algorithm}
    \caption{OMP Algorithm}
  \begin{algorithmic}[1]
    \Require Decomposition of signal $x$
    \Input signal $x \in \mathcal{R}^{m}$, Dictionary $\mathcal{G} \in \mathcal{R}^{m \times n}$, $\hat{x} = \emptyset$
    \Output Decomposed signal $\hat{x}_{\text{est}}$ after $k$ iteration, Residual $R^{(k)}$
    \State \textbf{Initialization} $R^{(0)} = x$
    \While{$i \leq k$}
      \State $l =  \argmax_{l = 1,\dots,l} |\left< g_l,R^{(i)} \right>|$ \label{omp_pick_correlation}
        \Comment{finding the atom in $\mathcal{G}$ with max correlation with residual.}
      \State $R^{(i+1)} = R{(i)}-a_l g_l^{(i)}$
      \State $\hat{x} = \hat{x}+\langle R^{(i)}, g_{l}^{(i)} \rangle g_{l}^{(i)}$
      \State $i = i + 1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

At \cref{omp_pick_correlation}, the algorithm chooses the column of the dictionary that matches the best the residual. This is where we thought this way of doing would serve our "add an element to one of the supports" algorithm.

% XXX define PALMTREE
Our goal is to, between two iterations of the PALMTREE algorithm, add a new element to one of the supports. That would lead to an optimization-driven construction of the kernels instead of designing them by hand.

But we have been questioning ourselves on the fairness of choosing an element to be added based the mere partial gradient. We remind that the \ref{omp_pick_correlation} is actually computing a gradient:

\begin{align*}
l =& \argmax \left|\left< g_l,R^{(i)} \right>\right| \\
=& \argmax \left| \mathcal{G}^TR^{(i)}\right| \\
=& \argmax \left| \mathcal{G}^T(\mathcal{G}\alpha - u)\right| \\
\end{align*}
So: does the partial gradient \ref{eq_partial_gradient} give 
\begin{align*}
\Phi((h^e)_{e \in \edges}) = \left\| \sum_{f\in\leaves} \code^f * h^{\CC(r,f)} \right\|^2
\end{align*}

\begin{align*} 
\nabla_{h^{e'}}\Phi((h^e)_{e \in \edges}) = 2 \widetilde{H^{e'}} * (h^{e'}*H^{e'}-y^{e'})
\end{align*} \label{eq_partial_gradient}



