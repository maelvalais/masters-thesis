\input{packages_config.tex} % <-- all the \usepackages
\author{Maël Valais}
\date{Updated on \today}
\title{Master's Thesis – Optimization of dictionaries structured in convolutional trees for sparse image representation}
\begin{document}
%\input{titlepage} % Uncomment to show title page

\section{The need for sparse representations}
A sparse signal over some representation means that it can be written with as few information as possible – in other words, sparse means with many zeros. Many applications spanning from machine learning to image denoising heavily rely on the property that we can summarize – or more precisely approximate, or factorize – any signal using a proper representation. The job of transforming the raw\footnote{Any raw signal is actually represented by the canonical basis} signal to its sparse counterpart takes the form of an operator, often called dictionary or transform.

The Fourier transform is an ideal example of such operators. In figure\ref{exple_fourier}, the left picture $y$ has been represented using the Fourier basis, which can be written as a matrix $D$. The right picture shows the coefficients $x$ such that
$$x = D^Ty$$
\begin{figure}  \label{exple_fourier}
\subcaptionbox{A picture featuring the definition of an isometry on a blackboard}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_spacial.jpg}}
 \subcaptionbox{Frequential representation of the blackboard using Fourier transform. Middle coefficients are coding for general and constant features of the picture while the corner values are coding for the details. The multiple white lines, spanning from the middle to the borders, outline the discontinuities caused by the edges of the blackboard.}%
  [.49\linewidth]{\includegraphics[width=0.49\textwidth]{figures/exple_fourier_frequen.jpg}}
\end{figure}
But Fourier is quite different to our convolutional tree structure. Note that Fourier representation is exact thanks to its analytic form. This allows fast implementations, meaning that the transform is fast (computing the above picture using \texttt{fft2} on matlab is extremely fast). Also, due to the fact that it is a basis (thus bijective), the inverse transform exists and is unique. % XXX useless? 

Also, we can see that the right picture doesn't seem to be sparse: many coefficients are non-zeros. Fourier is not adapted to every kind of signals, but can give good results in some cases.

Many other non-adaptative dictionaries exist, like the cosine transform (used by JPEG), the wavelet transform (used by JPEG2000, heavily used in theaters) or the curvelet transform. And as for the Fourier transform, these operators are known to be fast.

\section{Adaptative dictionaries}
As mentioned, analytically-based dictionaries do not always result to sparse representations, although allowing low computational time. Dictionary learning tries to address the adaptativeness issue; instead of using basis, we allow ourselves to use overcomplete dictionaries made of learned images (the atoms).
\begin{figure}
\includegraphics[width=0.5\textwidth]{figures/sparsity-matrix.pdf}
\end{figure}
\ac{K-SVD} is a very well known algorithm for dictionary learning. Instead of having a pre-determined $D$, we are going to learn it. Many of the concepts behind the PALMTREE algorithm developed by Olivier Chabiron come from the K-SVD algorithm.

\section{K-SVD algorithm}

K-SVD basically optimizes the dictionary by alternatively optimizing $x$ and $D$. Optimizing $x$ is the sparse coding phase, and optimizing $D$ is the dictionary update phase.

\section{PALMTREE algorithm}

The current work, mainly developed during Olivier Chabiron's PhD, gives a theoretical frae to the (FTL) problem. The (FTL) problem aims at approximating an ideal dictionary $D^{ideal}$ (which can be easily understood as a matrix) made of ideal atoms $a_i$ on its columns. The approximated dictionary will be called $D$ and the atoms formed by its columns $(H^f)_{f \in \leaves}$. 

Instead of using a standard matrix-vector product to compute $Dx$ and $D^Ty$, the operator $D(x)$ is defined as a convolutional tree:
$$D(x) = \sum_{f\in\leaves} H^f * x^f$$
The columns of $D$ – the atoms $H^f$ – are simply defined by the successive convolution of kernels $h^i$ from the root to the leaf
$$H^f = h^1 * h^2 \dots * h^k$$
% XXX code ?

To sum up the convolutional tree model, here are some outlines of the $D$ operator:
\begin{itemize}
\item a standard dictionary would require many translated versions of the same atom; here, the convolution $D*x$ does the job 
\item because of the convolution $D*x$, each "component" $x^f$ of the code $x$ is the same size as the target image $y$. We will talk about this block-wise code a bit farther
\end{itemize}


\begin{equation*} \begin{aligned}
Dx \approx y
\end{aligned} \end{equation*}


\begin{figure} \centering
\includegraphics[width=0.9\textwidth]{figures/matrix-vs-tree.pdf} \label{matrix_vs_tree}
\end{figure}



In this work, we will generally stay on the non-noisy case, leaving noisiness to further work.

Algorithm PALMTREE: a Gauss-Seidel algorithm (in the sense that we 

In dictionary learning, we call the columns of $D$ the \emph{atoms}.


This experiment aims to simulate the choice of an atom by the OMP algorithm. As a short reminder, here is the step we are studying:
\begin{algorithm}
    \caption{OMP Algorithm}
  \begin{algorithmic}[1]
    \Require Decomposition of signal $x$
    \Input signal $x \in \mathcal{R}^{m}$, Dictionary $\mathcal{G} \in \mathcal{R}^{m \times n}$, $\hat{x} = \emptyset$
    \Output Decomposed signal $\hat{x}_{\text{est}}$ after $k$ iteration, Residual $R^{(k)}$
    \State \textbf{Initialization} $R^{(0)} = x$
    \While{$i \leq k$}
      \State $l =  \argmax_{l = 1,\dots,l} |\left< g_l,R^{(i)} \right>|$ \label{omp_pick_correlation}
        \Comment{finding the atom in $\mathcal{G}$ with max correlation with residual.}
      \State $R^{(i+1)} = R{(i)}-a_l g_l^{(i)}$
      \State $\hat{x} = \hat{x}+\langle R^{(i)}, g_{l}^{(i)} \rangle g_{l}^{(i)}$
      \State $i = i + 1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

At \cref{omp_pick_correlation}, the algorithm chooses the column of the dictionary that matches the best the residual. This is where we thought this way of doing would serve our "add an element to one of the supports" algorithm.

% XXX define PALMTREE
Our goal is to, between two iterations of the PALMTREE algorithm, add a new element to one of the supports. That would lead to an optimization-driven construction of the kernels instead of designing them by hand.

But we have been questioning ourselves on the fairness of choosing an element to be added based the mere partial gradient. We remind that the \ref{omp_pick_correlation} is actually computing a gradient:

\begin{align*}
l =& \argmax \left|\left< g_l,R^{(i)} \right>\right| \\
=& \argmax \left| \mathcal{G}^TR^{(i)}\right| \\
=& \argmax \left| \mathcal{G}^T(\mathcal{G}\alpha - u)\right| \\
\end{align*}
So: does the partial gradient \ref{eq_partial_gradient} give 
\begin{align*}
\Phi((h^e)_{e \in \edges}) = \left\| \sum_{f\in\leaves} \code^f * h^{\CC(r,f)} \right\|^2
\end{align*}
\begin{align*} 
\nabla_{h^{e'}}\Phi((h^e)_{e \in \edges}) = 2 \widetilde{H^{e'}} * (h^{e'}*H^{e'}-y^{e'})
\end{align*} \label{eq_partial_gradient}




\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_approx.png}
\end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_gradient_node_1.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.004325 & 9.6\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{1} edge. Note that for the "added point" RMSE, we took the minimum of all RMSE for every point possibly added. Here, the RMSE is at best increased by 9.6\%.}
\end{table}


\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_gradient_node_2.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.004407 & 7.9\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{2} edge}
\end{table}


\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_gradient_node_3.png}
    \end{subfigure}
\end{figure}


\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.003973 & 16.9\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{3} edge}
\end{table}


\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_obj_matrix.png}
    \end{subfigure}
       \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_gradient_node_4.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.003790 & 20.8\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{4} edge}
\end{table}

% XXX Proof not finished; this proof might be useless/overkill in my thesis...
\begin{align*}
(\widehat{\widetilde{A}})_{m,n} =& \sum_{k=1}^M \sum_{l=1}^N \widehat{A}_{k,l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
=& \sum_{k=1}^M \sum_{l=1}^N A_{-k,-l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
\shortintertext{By changing variables $k'=-k$ and $l'=-l$, we get:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\shortintertext{And thanks to the $(M,N)$ periodicity of $A$, which means that $A_{i,j}=A_{i+kM,j+lN}$, $\forall (k,l) \in \mathbb{N}^2$, letting us with:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k'+M,l'+N} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
\shortintertext{With a second change of variables $k''=k'+M$ and $l''=l'+N$:}
=& \sum_{k''=-M+M}^{-1+M} \sum_{l''=-N+N}^{-1+N} A_{k'',l''} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
=& \sum_{k'=1}^{M} \sum_{l'=1}^{N} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\end{align*}


%---------- USELESS \\
Note: I first wrote it using indices running from 1 to N, but it creates a kind of "glitch" that breaks the symmetry... With a signal $x$ that spans in $[1;3]$. As this signal is periodical, you get the same values at $[1+3;3+3]=[4;6]$. The generalization gives $x_i = x_{i+kN}$ with $k\in\mathbb{N}$. But starting at 1 prevents from generalizing to negative $k$.
\begin{table}\centering\begin{tabular}{lllllllllll}\hline 
-3 & -2 & -1 & \multicolumn{1}{l|}{0} & 1 & 2 & \multicolumn{1}{l|}{3} & 4 & 5 & \multicolumn{1}{l|}{6} & 7 \\ \hline
\multicolumn{4}{c}{Signal?} & \multicolumn{3}{c}{Signal} & \multicolumn{3}{c}{Signal} & 
\end{tabular}\end{table}
Indeed, $[1-3;3-3]=[2;0]$
% ------------------------ \\
\end{document}



