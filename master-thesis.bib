@article{bristow_optimization_2014,
	title = {Optimization {Methods} for {Convolutional} {Sparse} {Coding}},
	url = {http://arxiv.org/abs/1406.2407},
	abstract = {Sparse and convolutional constraints form a natural prior for many optimization problems that arise from physical processes. Detecting motifs in speech and musical passages, super-resolving images, compressing videos, and reconstructing harmonic motions can all leverage redundancies introduced by convolution. Solving problems involving sparse and convolutional constraints remains a difficult computational problem, however. In this paper we present an overview of convolutional sparse coding in a consistent framework. The objective involves iteratively optimizing a convolutional least-squares term for the basis functions, followed by an L1-regularized least squares term for the sparse coefficients. We discuss a range of optimization methods for solving the convolutional sparse coding objective, and the properties that make each method suitable for different applications. In particular, we concentrate on computational complexity, speed to \{{\textbackslash}epsilon\} convergence, memory usage, and the effect of implied boundary conditions. We present a broad suite of examples covering different signal and application domains to illustrate the general applicability of convolutional sparse coding, and the efficacy of the available optimization methods.},
	urldate = {2016-03-03},
	journal = {arXiv:1406.2407 [cs]},
	author = {Bristow, Hilton and Lucey, Simon},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2407},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1406.2407 PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/3P6ICVV6/Bristow et Lucey - 2014 - Optimization Methods for Convolutional Sparse Codi.pdf:application/pdf;arXiv.org Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/XVPC8V8A/1406.html:text/html}
}

@inproceedings{liu_sparse_2015,
	title = {Sparse {Convolutional} {Neural} {Networks}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.html},
	urldate = {2016-03-03},
	author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Pensky, Marianna},
	year = {2015},
	pages = {806--814},
	file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/HVJAPJRA/Liu et al. - 2015 - Sparse Convolutional Neural Networks.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/KGZENXAT/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.html:text/html}
}

@unpublished{chabiron_optimization_2016,
	title = {Optimization of a {Fast} {Transform} {Structured} as a {Convolutional} {Tree}},
	url = {https://hal.archives-ouvertes.fr/hal-01258514},
	abstract = {To reduce the dimension of large datasets, it is common to express each vector of this dataset using few atoms of a redundant dictionary. In order to select these atoms, many models and algorithms have been proposed, leading to state-of-the-art performances in many machine learning, signal and image processing applications. The classical sparsifying algorithms compute at each iteration matrix-vector multiplications where the matrix contains the atoms of the dictionary. As a consequence, the numerical complexity of the sparsifying algorithm is always proportional to the numerical complexity of the matrix-vector multiplication. In some applications, the matrix-vector multiplications can be computed using handcrafted fast transforms (such as the Fourier or the wavelet transforms). However, the complexity of the matrix-vector multiplications very often limits the capacities of the sparsifying algorithms. It is particularly the case when the transform is learned from the data. In order to avoid this limitation, we study a strategy to optimize convolutions of sparse kernels living on the edges of a tree. These convolutions define a fast transform (algorithmically similar to a fast wavelet transform) that can approximate atoms prescribed by the user. The optimization problem associated with the learning of these fast transforms is smooth but can be strongly non-convex. We propose in this paper a proximal alternating linearized minimization algorithm (PALMTREE) allowing Curvelet or Wavelet-packet transforms to be approximated with excellent performance. Empirically, the profile of the objective function associated with this optimization problem has a small number of critical values with large watershed. This confirms that the resulting fast transforms can be optimized efficiently, which opens many theoretical and applicative perspectives.},
	urldate = {2016-03-08},
	author = {Chabiron, Olivier and Malgouyres, François and Wendt, Herwig and Tourneret, Jean-Yves},
	month = jan,
	year = {2016},
	note = {working paper or preprint},
	keywords = {data analysis, data representation, Dictionary learning, Fast transform},
	file = {FTO.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/6HN3EZSA/FTO.pdf:application/pdf;HAL Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/HUKK457Q/hal-01258514.html:text/html}
}

@article{rubinstein_dictionaries_2010,
	title = {Dictionaries for {Sparse} {Representation} {Modeling}},
	volume = {98},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2010.2040551},
	abstract = {Sparse and redundant representation modeling of data assumes an ability to describe signals as linear combinations of a few atoms from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. In general, the choice of a proper dictionary can be done using one of two ways: i) building a sparsifying dictionary based on a mathematical model of the data, or ii) learning a dictionary to perform best on a training set. In this paper we describe the evolution of these two paradigms. As manifestations of the first approach, we cover topics such as wavelets, wavelet packets, contourlets, and curvelets, all aiming to exploit 1-D and 2-D mathematical models for constructing effective dictionaries for signals and images. Dictionary learning takes a different route, attaching the dictionary to a set of examples it is supposed to serve. From the seminal work of Field and Olshausen, through the MOD, the K-SVD, the Generalized PCA and others, this paper surveys the various options such training has to offer, up to the most recent contributions and structures.},
	number = {6},
	journal = {Proceedings of the IEEE},
	author = {Rubinstein, R. and Bruckstein, A. M. and Elad, M.},
	month = jun,
	year = {2010},
	keywords = {Dictionaries, Dictionary learning, Displays, Harmonic analysis, Joining processes, mathematical data model, Mathematical model, Principal component analysis, redundant signal representation modeling, Sampling methods, signal approximation, Signal processing, signal representation, Signal representations, signal sampling, sparse coding, sparse representation, sparse signal representation modeling, training set, Wavelet packets, wavelet transforms},
	pages = {1045--1057},
	file = {IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/QR7FHU6H/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/B5NI7R8F/Rubinstein et al. - 2010 - Dictionaries for Sparse Representation Modeling.pdf:application/pdf}
}

@article{aharon_k-svd:_2006,
	title = {K-{SVD}: {An} {Algorithm} for {Designing} {Overcomplete} {Dictionaries} for {Sparse} {Representation}},
	volume = {54},
	issn = {1053-587X},
	shorttitle = {-{SVD}},
	doi = {10.1109/TSP.2006.881199},
	abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Aharon, M. and Elad, M. and Bruckstein, A.},
	month = nov,
	year = {2006},
	keywords = {Algorithm design and analysis, Atom decomposition, Basis pursuit, Clustering algorithms, codebook, Dictionaries, dictionary, Feature extraction, FOCUSS, gain-shape VQ, image coding, image data, image representation, Inverse problems, Iterative algorithms, iterative method, iterative methods, K-means clustering process, K-SVD, linear transforms, matching pursuit, Matching pursuit algorithms, overcomplete dictionary, Prototypes, Pursuit algorithms, Signal design, signals sparse representation, singular value decomposition, sparse coding, sparse representation, sparsity constraints, training, transforms, vector quantization},
	pages = {4311--4322},
	file = {IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/NVEWENIX/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/JNV66I8D/Aharon et al. - 2006 - -SVD An Algorithm for Designing Overcomplete Dict.pdf:application/pdf}
}

@article{chabiron_toward_2015,
	title = {Toward {Fast} {Transform} {Learning}},
	volume = {114},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1007/s11263-014-0771-z},
	doi = {10.1007/s11263-014-0771-z},
	abstract = {This paper introduces a new dictionary learning strategy based on atoms obtained by translating the composition of KKK convolutions with SSS-sparse kernels of known support. The dictionary update step associated with this strategy is a non-convex optimization problem. We propose a practical formulation of this problem and introduce a Gauss–Seidel type algorithm referred to as alternative least square algorithm for its resolution. The search space of the proposed algorithm is of dimension KSKSKS, which is typically smaller than the size of the target atom and much smaller than the size of the image. Moreover, the complexity of this algorithm is linear with respect to the image size, allowing larger atoms to be learned (as opposed to small patches). The conducted experiments show that we are able to accurately approximate atoms such as wavelets, curvelets, sinc functions or cosines for large values of K. The proposed experiments also indicate that the algorithm generally converges to a global minimum for large values of KKK and SSS.},
	language = {en},
	number = {2-3},
	urldate = {2016-04-01},
	journal = {Int J Comput Vis},
	author = {Chabiron, Olivier and Malgouyres, François and Tourneret, Jean-Yves and Dobigeon, Nicolas},
	month = sep,
	year = {2015},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Imaging, Vision, Pattern Recognition and Graphics, Dictionary learning, Fast transform, Gauss–Seidel, Global optimization, Image Processing and Computer Vision, Matrix factorization, Pattern Recognition, sparse representation},
	pages = {195--216},
	file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/I2SJUIJJ/Chabiron et al. - 2014 - Toward Fast Transform Learning.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/S3I7N7W9/10.html:text/html}
}

@article{sulam_trainlets:_2016,
	title = {Trainlets: {Dictionary} {Learning} in {High} {Dimensions}},
	volume = {64},
	issn = {1053-587X},
	shorttitle = {Trainlets},
	doi = {10.1109/TSP.2016.2540599},
	abstract = {Sparse representation has shown to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results in a wide variety of tasks. These methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped Wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the base dictionary within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call Trainlets.},
	number = {12},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sulam, J. and Ophir, B. and Zibulevsky, M. and Elad, M.},
	month = jun,
	year = {2016},
	keywords = {Adaptation models, Context, contourlets, cropped wavelet, Dictionary learning, Double-sparsity, Image processing, K-SVD, Mathematical model, on-line learning, Signal processing algorithms, training, trainlets},
	pages = {3180--3193},
	file = {IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/95D3A9BD/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/7EAWR2ZT/Sulam et al. - 2016 - Trainlets Dictionary Learning in High Dimensions.pdf:application/pdf}
}

@article{donoho_stable_2006,
	title = {Stable recovery of sparse overcomplete representations in the presence of noise},
	volume = {52},
	issn = {0018-9448},
	doi = {10.1109/TIT.2005.860430},
	abstract = {Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.},
	number = {1},
	journal = {IEEE Transactions on Information Theory},
	author = {Donoho, D. L. and Elad, M. and Temlyakov, V. N.},
	month = jan,
	year = {2006},
	keywords = {approximation theory, Basis pursuit, Dictionaries, greedy approximation, greedy approximation algorithm, incoherent dictionary, iterative methods, Kruskal rank, Linear algebra, matching pursuit, Matching pursuit algorithms, Noise generators, Noise level, noisy data, optimal sparse decomposition, overcomplete representation, signal denoising, Signal processing, Signal processing algorithms, signal processing theory, signal representation, Signal representations, sparse overcomplete representation, sparse representation, Stability, stability, stable recovery, stepwise regression, superresolution, superresolution signal, time-frequency analysis, Vectors},
	pages = {6--18},
	file = {IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/424IN4K7/Donoho et al. - 2006 - Stable recovery of sparse overcomplete representat.html:text/html;IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/VDT966GT/Donoho et al. - 2006 - Stable recovery of sparse overcomplete representat.pdf:application/pdf}
}

@phdthesis{chabiron_apprentissage_2015,
	type = {phd},
	title = {Apprentissage d'arbres de convolutions pour la représentation parcimonieuse},
	url = {http://thesesups.ups-tlse.fr/2994/},
	abstract = {Le domaine de l'apprentissage de dictionnaire est le sujet d'attentions croissantes durant cette dernière décennie. L'apprentissage de dictionnaire est une approche adaptative de la représentation parcimonieuse de données. Les méthodes qui constituent l'état de l'art en DL donnent d'excellentes performances en approximation et débruitage. Cependant, la complexité calculatoire associée à ces méthodes restreint leur utilisation à de toutes petites images ou "patchs". Par conséquent, il n'est pas possible d'utiliser l'apprentissage de dictionnaire pour des applications impliquant de grandes images, telles que des images de télédétection. Dans cette thèse, nous proposons et étudions un modèle original d'apprentissage de dictionnaire, combinant une méthode de décomposition des images par convolution et des structures d'arbres de convolution pour les dictionnaires. Ce modèle a pour but de fournir des algorithmes efficaces pour traiter de grandes images, sans les décomposer en patchs. Dans la première partie, nous étudions comment optimiser une composition de convolutions de noyaux parcimonieux, un problème de factorisation matricielle non convexe. Ce modèle est alors utilisé pour construire des atomes de dictionnaire. Dans la seconde partie, nous proposons une structure de dictionnaire basée sur des arbres de convolution, ainsi qu'un algorithme de mise à jour de dictionnaire adapté à cette structure. Enfin, une étape de décomposition parcimonieuse est ajoutée à cet algorithme dans la dernière partie. À chaque étape de développement de la méthode, des expériences numériques donnent un aperçu de ses capacités d'approximation.},
	urldate = {2016-06-21},
	school = {Université de Toulouse, Université Toulouse III - Paul Sabatier},
	author = {Chabiron, Olivier},
	month = oct,
	year = {2015},
	file = {2015TOU30213.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/U7WNZ576/2015TOU30213.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/ZZVRX7GX/Chabiron - 2015 - Apprentissage d'arbres de convolutions pour la rep.html:text/html}
}

@article{condat_fast_2015,
	title = {Fast projection onto the simplex and the {\textbackslash}pmb \{l\}\_{\textbackslash}mathbf \{1\} ball},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/article/10.1007/s10107-015-0946-6},
	doi = {10.1007/s10107-015-0946-6},
	abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an l1l1l\_1-norm ball. It can be viewed as a Gauss–Seidel-like variant of Michelot’s variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
	language = {en},
	urldate = {2016-05-04},
	journal = {Math. Program.},
	author = {Condat, Laurent},
	month = sep,
	year = {2015},
	keywords = {49M30, 65C60, 65K05, 90C25, Calculus of Variations and Optimal Control; Optimization, Combinatorics, Large-scale optimization, Mathematical Methods in Physics, Mathematics of Computing, Numerical Analysis, Simplex, Theoretical, Mathematical and Computational Physics},
	pages = {1--11},
	file = {condat2015.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/VWJU4GMB/condat2015.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/FPZN74ZM/10.html:text/html}
}

@article{held_validation_1974,
	title = {Validation of subgradient optimization},
	volume = {6},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/article/10.1007/BF01580223},
	doi = {10.1007/BF01580223},
	abstract = {The “relaxation” procedure introduced by Held and Karp for approximately solving a large linear programming problem related to the traveling-salesman problem is refined and studied experimentally on several classes of specially structured large-scale linear programming problems, and results on the use of the procedure for obtaining exact solutions are given. It is concluded that the method shows promise for large-scale linear programming},
	language = {en},
	number = {1},
	urldate = {2016-05-04},
	journal = {Mathematical Programming},
	author = {Held, Michael and Wolfe, Philip and Crowder, Harlan P.},
	month = dec,
	year = {1974},
	keywords = {Calculus of Variations and Optimal Control, Combinatorics, Mathematical and Computational Physics, Mathematical Methods in Physics, Mathematics of Computing, Numerical Analysis, Numerical and Computational Methods, Operation Research/Decision Theory, Optimization},
	pages = {62--88},
	annote = {Pour comprendre la projection l1},
	file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/7IQDQV9F/Held et al. - 1974 - Validation of subgradient optimization.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/2BMKN3FI/10.html:text/html}
}

@article{herzet_exact_2013,
	title = {Exact {Recovery} {Conditions} for {Sparse} {Representations} with {Partial} {Support} {Information}},
	url = {http://arxiv.org/abs/1305.4008},
	abstract = {We address the exact recovery of a k-sparse vector in the noiseless setting when some partial information on the support is available. This partial information takes the form of either a subset of the true support or an approximate subset including wrong atoms as well. We derive a new sufficient and worst-case necessary (in some sense) condition for the success of some procedures based on lp-relaxation, Orthogonal Matching Pursuit (OMP) and Orthogonal Least Squares (OLS). Our result is based on the coherence "mu" of the dictionary and relaxes the well-known condition mu{\textless}1/(2k-1) ensuring the recovery of any k-sparse vector in the non-informed setup. It reads mu{\textless}1/(2k-g+b-1) when the informed support is composed of g good atoms and b wrong atoms. We emphasize that our condition is complementary to some restricted-isometry based conditions by showing that none of them implies the other. Because this mutual coherence condition is common to all procedures, we carry out a finer analysis based on the Null Space Property (NSP) and the Exact Recovery Condition (ERC). Connections are established regarding the characterization of lp-relaxation procedures and OMP in the informed setup. First, we emphasize that the truncated NSP enjoys an ordering property when p is decreased. Second, the partial ERC for OMP (ERC-OMP) implies in turn the truncated NSP for the informed l1 problem, and the truncated NSP for p{\textless}1.},
	urldate = {2016-05-27},
	journal = {arXiv:1305.4008 [cs, math]},
	author = {Herzet, C. and Soussen, C. and Idier, J. and Gribonval, R.},
	month = may,
	year = {2013},
	note = {arXiv: 1305.4008},
	keywords = {Computer Science - Information Theory},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1211.7283},
	file = {arXiv.org Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/ZHXG2KBU/Herzet et al. - 2013 - Exact Recovery Conditions for Sparse Representatio.html:text/html}
}

@article{tropp_signal_2007,
	title = {Signal {Recovery} {From} {Random} {Measurements} {Via} {Orthogonal} {Matching} {Pursuit}},
	volume = {53},
	issn = {0018-9448},
	doi = {10.1109/TIT.2007.909108},
	abstract = {This paper demonstrates theoretically and empirically that a greedy algorithm called orthogonal matching pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m ln d) random linear measurements of that signal. This is a massive improvement over previous results, which require O(m2) measurements. The new results for OMP are comparable with recent results for another approach called basis pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.},
	number = {12},
	journal = {IEEE Transactions on Information Theory},
	author = {Tropp, J. A. and Gilbert, A. C.},
	month = dec,
	year = {2007},
	keywords = {Algorithms, approximation, Basis pursuit, Blood, Compressed sensing, greedy algorithm, greedy algorithms, group testing, iterative methods, Matching pursuit algorithms, Mathematics, orthogonal matching pursuit, Performance evaluation, random linear measurements, Reliability theory, Signal processing, signal recovery, sparse approximation, Testing, time-frequency analysis, Vectors},
	pages = {4655--4666},
	file = {IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/HI7HTUNJ/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/XGRE8C4E/Tropp et Gilbert - 2007 - Signal Recovery From Random Measurements Via Ortho.pdf:application/pdf}
}

@article{candes_fast_2006,
	title = {Fast {Discrete} {Curvelet} {Transforms}},
	volume = {5},
	issn = {1540-3459, 1540-3467},
	url = {http://epubs.siam.org/doi/abs/10.1137/05064182X},
	doi = {10.1137/05064182X},
	language = {en},
	number = {3},
	urldate = {2016-05-17},
	journal = {Multiscale Modeling \& Simulation},
	author = {Candès, Emmanuel and Demanet, Laurent and Donoho, David and Ying, Lexing},
	month = jan,
	year = {2006},
	pages = {861--899}
}