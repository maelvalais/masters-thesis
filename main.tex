\input{packages_config.tex} % <-- all the \usepackages
\author{Maël Valais}
\date{Updated on \today}
\title{Master's Thesis – Optimization of dictionaries structured in convolutional trees for sparse image representation}
\begin{document}
%\input{titlepage} % Uncomment to show title page

\section{Subject and goals}

The current work, mainly developed during Olivier Chabiron's PhD, gives a theoretical frame to the (FTL) problem. The (FTL) problem aims at approximating an ideal dictionary $D^{ideal}$ (which can be easily understood as a matrix) made of ideal atoms $a_i$ on its columns. The approximated dictionary will be called $D$ and the atoms formed by its columns $(H^f)_{f \in \leaves}$. 

Instead of using a standard matrix-vector product to compute $Dx$ and $D^Ty$, the operator $D(x)$ is defined as a convolutional tree:
$$D(x) = \sum_{f\in\leaves} H^f x^f$$
The columns of $D$ – the atoms $H^f$ – are simply defined by the successive convolution of kernels $h^i$ from the root to the leaf
$$H^f = h^1 * h^2 \dots * h^k$$

To sum up the convolutional tree model, here are some outlines of the $D$ operator:
\begin{itemize}
\item a standard dictionary
\item azdadz
\end{itemize}

\begin{equation*} \begin{aligned}
Dx \approx y
\end{aligned} \end{equation*}


\begin{figure} \centering
\includegraphics[width=0.9\textwidth]{figures/matrix-vs-tree.pdf} \label{matrix_vs_tree}
\end{figure}


Many image processing techniques are based on... representing the signal ...

We aim... 

Now that it has been proven that the objective function, , actually allows to find local minimums that are close enough to the actual minimum.


The idea behind dictionaries
Say $y$ is a signal we want to ...
We want to find a transform/dictionary D such that, when applied to a code (think of the "transformed" signal) will give an approximated signal $\hat{y}$:

$$D\alpha=\hat{y} \approx y$$

In this work, we will generally stay on the non-noisy case, leaving noisiness to further work.

Algorithm PALMTREE: a Gauss-Seidel algorithm (in the sense that we 

In dictionary learning, we call the columns of $D$ the \emph{atoms}.


This experiment aims to simulate the choice of an atom by the OMP algorithm. As a short reminder, here is the step we are studying:
\begin{algorithm}
    \caption{OMP Algorithm}
  \begin{algorithmic}[1]
    \Require Decomposition of signal $x$
    \Input signal $x \in \mathcal{R}^{m}$, Dictionary $\mathcal{G} \in \mathcal{R}^{m \times n}$, $\hat{x} = \emptyset$
    \Output Decomposed signal $\hat{x}_{\text{est}}$ after $k$ iteration, Residual $R^{(k)}$
    \State \textbf{Initialization} $R^{(0)} = x$
    \While{$i \leq k$}
      \State $l =  \argmax_{l = 1,\dots,l} |\left< g_l,R^{(i)} \right>|$ \label{omp_pick_correlation}
        \Comment{finding the atom in $\mathcal{G}$ with max correlation with residual.}
      \State $R^{(i+1)} = R{(i)}-a_l g_l^{(i)}$
      \State $\hat{x} = \hat{x}+\langle R^{(i)}, g_{l}^{(i)} \rangle g_{l}^{(i)}$
      \State $i = i + 1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

At \cref{omp_pick_correlation}, the algorithm chooses the column of the dictionary that matches the best the residual. This is where we thought this way of doing would serve our "add an element to one of the supports" algorithm.

% XXX define PALMTREE
Our goal is to, between two iterations of the PALMTREE algorithm, add a new element to one of the supports. That would lead to an optimization-driven construction of the kernels instead of designing them by hand.

But we have been questioning ourselves on the fairness of choosing an element to be added based the mere partial gradient. We remind that the \ref{omp_pick_correlation} is actually computing a gradient:

\begin{align*}
l =& \argmax \left|\left< g_l,R^{(i)} \right>\right| \\
=& \argmax \left| \mathcal{G}^TR^{(i)}\right| \\
=& \argmax \left| \mathcal{G}^T(\mathcal{G}\alpha - u)\right| \\
\end{align*}
So: does the partial gradient \ref{eq_partial_gradient} give 
\begin{align*}
\Phi((h^e)_{e \in \edges}) = \left\| \sum_{f\in\leaves} \code^f * h^{\CC(r,f)} \right\|^2
\end{align*}
\begin{align*} 
\nabla_{h^{e'}}\Phi((h^e)_{e \in \edges}) = 2 \widetilde{H^{e'}} * (h^{e'}*H^{e'}-y^{e'})
\end{align*} \label{eq_partial_gradient}






\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_approx.png}
\end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node1_gradient_node_1.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.004325 & 9.6\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{1} edge. Note that for the "added point" RMSE, we took the minimum of all RMSE for every point possibly added. Here, the RMSE is at best increased by 9.6\%.}
\end{table}


\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node2_gradient_node_2.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.004407 & 7.9\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{2} edge}
\end{table}


\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_obj_matrix.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node3_gradient_node_3.png}
    \end{subfigure}
\end{figure}


\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.003973 & 16.9\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{3} edge}
\end{table}


\begin{figure}[!h]\centering
    \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_target.png}
    \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}\centering
\includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_approx.png}
\end{subfigure}
   \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_obj_matrix.png}
    \end{subfigure}
       \begin{subfigure}[b]{0.49\textwidth}\centering
    \includegraphics[width=\textwidth]{figures/xp/xp_128x128_sc2_angl1_K3_S3_node4_gradient_node_4.png}
    \end{subfigure}
\end{figure}

\begin{table}[!h]\centering
\begin{tabular}{@{}lll@{}}\toprule
 & RMSE & Relative RMSE \\ \midrule
Before & 0.004786 & 0\% \\
After & 0.003790 & 20.8\% \\ \bottomrule
\end{tabular}
\caption{RMSE comparison when adding to the support on the \nth{4} edge}
\end{table}

% XXX Proof not finished; this proof might be useless/overkill in my thesis...
\begin{align*}
(\widehat{\widetilde{A}})_{m,n} =& \sum_{k=1}^M \sum_{l=1}^N \widehat{A}_{k,l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
=& \sum_{k=1}^M \sum_{l=1}^N A_{-k,-l} e^{-2i\pi (k\frac{m}{M}+l \frac{n}{N})}\\
\shortintertext{By changing variables $k'=-k$ and $l'=-l$, we get:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\shortintertext{And thanks to the $(M,N)$ periodicity of $A$, which means that $A_{i,j}=A_{i+kM,j+lN}$, $\forall (k,l) \in \mathbb{N}^2$, letting us with:}
=& \sum_{k'=-M}^{-1} \sum_{l'=-N}^{-1} A_{k'+M,l'+N} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
\shortintertext{With a second change of variables $k''=k'+M$ and $l''=l'+N$:}
=& \sum_{k''=-M+M}^{-1+M} \sum_{l''=-N+N}^{-1+N} A_{k'',l''} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}\\
=& \sum_{k'=1}^{M} \sum_{l'=1}^{N} A_{k',l'} e^{2i\pi (k'\frac{m}{M}+l' \frac{n}{N})}
\end{align*}


%---------- USELESS \\
Note: I first wrote it using indices running from 1 to N, but it creates a kind of "glitch" that breaks the symmetry... With a signal $x$ that spans in $[1;3]$. As this signal is periodical, you get the same values at $[1+3;3+3]=[4;6]$. The generalization gives $x_i = x_{i+kN}$ with $k\in\mathbb{N}$. But starting at 1 prevents from generalizing to negative $k$.
\begin{table}\centering\begin{tabular}{lllllllllll}\hline 
-3 & -2 & -1 & \multicolumn{1}{l|}{0} & 1 & 2 & \multicolumn{1}{l|}{3} & 4 & 5 & \multicolumn{1}{l|}{6} & 7 \\ \hline
\multicolumn{4}{c}{Signal?} & \multicolumn{3}{c}{Signal} & \multicolumn{3}{c}{Signal} & 
\end{tabular}\end{table}
Indeed, $[1-3;3-3]=[2;0]$
% ------------------------ \\
\end{document}



